{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "➡️ Make sure that you have read the **[rules for hand-in assignments](https://www.ida.liu.se/~TDDE16/exam.en.shtml#handins)** and the **[policy on cheating and plagiarism](https://www.ida.liu.se/~TDDE16/exam.en.shtml#cheating)** before starting with this lab.\n",
    "\n",
    "➡️ Make sure you fill in any cells (and _only_ those cells) that say **`YOUR CODE HERE`** or **YOUR ANSWER HERE**, and do _not_ modify any of the other cells.\n",
    "\n",
    "➡️ **Before you submit your lab, make sure everything runs as expected.** For this, _restart the kernel_ and _run all cells_ from top to bottom. In Jupyter Notebook version 7 or higher, you can do this via \"Run$\\rightarrow$Restart Kernel and Run All Cells...\" in the menu (or the \"⏩\" button in the toolbar).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L4: Information extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information extraction (IE) is the task of identifying named entities and semantic relations between these entities in text data. In this lab we will focus on two sub-tasks in IE, **named entity recognition** (identifying mentions of entities) and **entity linking** (matching these mentions to entities in a knowledge base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c78909256555edff5cdb9a87e9760e4",
     "grade": false,
     "grade_id": "cell-85c6255538d879ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define some helper functions that are used in this notebook\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def success():\n",
    "    display(HTML('<div class=\"alert alert-success\"><strong>Solution appears correct!</strong></div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading spaCy. However, the data that we will be using has been tokenized following the conventions of the [Penn Treebank](ftp://ftp.cis.upenn.edu/pub/treebank/public_html/tokenization.html), and we need to prevent spaCy from using its own tokenizer on top of this. We therefore override spaCy&rsquo;s tokenizer with the default one that simply splits on whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c11c5b4c67f7121419c123d22ca1c04",
     "grade": false,
     "grade_id": "cell-3aa55cdb0a00a39c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main data set for this lab is a collection of news wire articles in which mentions of named entities have been annotated with page names from the [English Wikipedia](https://en.wikipedia.org/wiki/). The next code cell loads the training and the development parts of the data into Pandas data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "537a3b88e8890f1c0cb2b63a27ccd3a9",
     "grade": false,
     "grade_id": "cell-6f2dc34d737a5d2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with bz2.open('ner-train.tsv.bz2', 'rt') as source:\n",
    "    df_train = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "with bz2.open('ner-dev.tsv.bz2', 'rt') as source:\n",
    "    df_dev = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in these two data frames corresponds to one mention of a named entity and has five columns:\n",
    "\n",
    "1. a unique identifier for the sentence containing the entity mention\n",
    "2. the pre-tokenized sentence, with tokens separated by spaces\n",
    "3. the start position of the token span containing the entity mention\n",
    "4. the end position of the token span (exclusive, as in Python list indexing)\n",
    "5. the entity label; either a Wikipedia page name or the generic label `--NME--`\n",
    "\n",
    "The following cell prints the first five samples from the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>beg</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000-000</td>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000-000</td>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000-000</td>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>United_Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000-001</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>--NME--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000-002</td>\n",
       "      <td>BRUSSELS 1996-08-22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Brussels</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_id                                          sentence  beg  end   \n",
       "0    0000-000  EU rejects German call to boycott British lamb .    0    1  \\\n",
       "1    0000-000  EU rejects German call to boycott British lamb .    2    3   \n",
       "2    0000-000  EU rejects German call to boycott British lamb .    6    7   \n",
       "3    0000-001                                   Peter Blackburn    0    2   \n",
       "4    0000-002                               BRUSSELS 1996-08-22    0    1   \n",
       "\n",
       "            label  \n",
       "0         --NME--  \n",
       "1         Germany  \n",
       "2  United_Kingdom  \n",
       "3         --NME--  \n",
       "4        Brussels  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample, we see that the first sentence is annotated with three entity mentions:\n",
    "\n",
    "* the span 0–1 &lsquo;EU&rsquo; is annotated as a mention but only labelled with the generic `--NME--`\n",
    "* the span 2–3 &lsquo;German&rsquo; is annotated with the page [Germany](http://en.wikipedia.org/wiki/Germany)\n",
    "* the span 6–7 &lsquo;British&rsquo; is annotated with the page [United_Kingdom](http://en.wikipedia.org/wiki/United_Kingdom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Evaluation measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To warm up, we ask you to write code to print the three measures that you will be using for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c23f8706ee19b4bd9ccea03fd3fece87",
     "grade": false,
     "grade_id": "cell-ca0fe4e7e601cd70",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluation_scores(gold, pred):\n",
    "    \"\"\"Print precision, recall, and F1 score.\n",
    "    \n",
    "    Arguments:\n",
    "        gold: The set with the gold-standard values.\n",
    "        pred: The set with the predicted values.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple or list containing the precision, recall, and F1 values\n",
    "        (in that order), computed based on the specified sets.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    precision = len(gold.intersection(pred))/len(pred)\n",
    "    recall = len(gold.intersection(pred))/len(gold)\n",
    "    F1= 2/((1/precision)+(1/recall))\n",
    "\n",
    "    return [precision, recall, F1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a convenience function that prints the scores nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8a0b8a24c5a862120c21a9df79e8115",
     "grade": false,
     "grade_id": "cell-97ba51a2487dc428",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def print_evaluation_scores(scores):\n",
    "    p, r, f = scores\n",
    "    print(f\"Precision: {p:.3f}, Recall: {r:.3f}, F1: {f:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your code, you can run the following cell. This should give you a precision of 60%, a recall of 100%, and an F1-value of 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0d52c9f78652c39e5c9e04b57b1ca8c",
     "grade": true,
     "grade_id": "cell-0de2d4e882169381",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.600, Recall: 1.000, F1: 0.750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\"><strong>Solution appears correct!</strong></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if the results match what is expected\n",
    "result = evaluation_scores(set(range(3)), set(range(5)))\n",
    "assert len(result) == 3, \"Should return exactly three scores\"\n",
    "print_evaluation_scores(result)\n",
    "assert np.isclose(result, (.6, 1.0, .75)).all(), \"Should be close to the expected values\"\n",
    "success()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Span recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first tasks that an information extraction system has to solve is to locate and classify (mentions of) named entities, such as persons and organizations. Here we will tackle the simpler task of recognizing **spans** of tokens that contain an entity mention, without the actual entity label.\n",
    "\n",
    "The English language model in spaCy features a full-fledged [named entity recognizer](https://spacy.io/usage/linguistic-features#named-entities) that identifies a variety of entities, and can be updated with new entity types by the user. Your task in this problem is to evaluate the performance of this component when predicting entity spans in the development data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1\n",
    "\n",
    "Start by implementing a generator function that yields the gold-standard spans in a given data frame.  (If you're not familiar with the `yield` keyword in Python, check out [this brief explanation](https://www.nbshare.io/notebook/851988260/Python-Yield/).)\n",
    "\n",
    "**Hint:** The Pandas method [`itertuples()`](https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.itertuples.html) is useful when iterating over the rows in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad2ad77ae4781b6ca59130065bfe996a",
     "grade": false,
     "grade_id": "cell-96b73b0db9000ea3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gold_spans(df):\n",
    "    \"\"\"Yield the gold-standard mention spans in a data frame.\n",
    "\n",
    "    Arguments:\n",
    "        df: A data frame.\n",
    "\n",
    "    Yields:\n",
    "        The gold-standard mention spans in the specified data frame as\n",
    "        triples consisting of the sentence id, start position, and end\n",
    "        position of each span.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    for row in df.itertuples():\n",
    "        yield((row[1],row[3],row[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤞 Test your code\n",
    "\n",
    "To test your code, you can run the following cell, which counts the spans yielded by your function when called on the development data (there should be 5,917 _unique_ triples), and checks if the first and last yielded triples are included in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fc4d57291bcca91a53033d9061fdba5",
     "grade": true,
     "grade_id": "cell-8cb736d325aa10ba",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\"><strong>Solution appears correct!</strong></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spans_dev_gold = set(gold_spans(df_dev))\n",
    "assert len(spans_dev_gold) == 5917, \"The number of unique returned triples is not correct.\"\n",
    "assert ('0946-000', 2, 3) in spans_dev_gold, \"The first expected triple is not included in the results.\"\n",
    "assert ('1161-010', 1, 3) in spans_dev_gold, \"The last expected triple is not included in the results.\"\n",
    "success()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2\n",
    "\n",
    "Your next task is to write code that calls spaCy to predict the named entities in the development data.  You should do this in form of a function that works the same as `gold_spans()`, but which returns the spans as predicted by spaCy instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(gold_spans(df_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c3dad78c16a22383473114f7e040fad",
     "grade": false,
     "grade_id": "cell-f5b568125d3e20e9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def pred_spans(df):\n",
    "    \"\"\"Run and evaluate spaCy's NER.\n",
    "\n",
    "    Arguments:\n",
    "        df: A data frame.\n",
    "\n",
    "    Yields:\n",
    "        The predicted mention spans in the specified data frame as\n",
    "        triples consisting of the sentence id, start position, and end\n",
    "        position of each span.\n",
    "    \"\"\"\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        doc = nlp(row[2])\n",
    "        for ent in doc.ents:\n",
    "            yield((row[1],ent.start, ent.end)) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤞 Test your code\n",
    "\n",
    "The following cell runs the prediction and reports the evaluation measures. The expected precision is above 50%, with a recall above 70% and an F1-score around 60%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f858e34608568c914e9196268e4f5944",
     "grade": true,
     "grade_id": "cell-a61480e94bbc6fc7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.519, Recall: 0.717, F1: 0.602\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\"><strong>Solution appears correct!</strong></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spans_dev_pred = set(pred_spans(df_dev))\n",
    "spans_dev_gold = set(gold_spans(df_dev))\n",
    "scores = evaluation_scores(spans_dev_gold, spans_dev_pred)\n",
    "print_evaluation_scores(scores)\n",
    "assert scores[0] > .50, \"Precision should be above 50%.\"\n",
    "assert scores[1] > .70, \"Recall should be above 70%.\"\n",
    "success()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you were able to see in Problem&nbsp;2, the span accuracy of the named entity recognizer is far from perfect. In particular, only slightly more than half of the predicted spans are correct according to the gold standard. Your next task is to analyse this result in more detail.\n",
    "\n",
    "Here is a function that prints the false positives as well as the false negatives spans for a data frame, given a reference set of gold-standard spans and a candidate set of predicted spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcd53e0a3866505e10dcd7e251c23afe",
     "grade": false,
     "grade_id": "cell-fbe0d078d84742d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def error_report(df, spans_gold, spans_pred):\n",
    "    false_pos = defaultdict(list)\n",
    "    for s, b, e in spans_pred - spans_gold:\n",
    "        false_pos[s].append((b, e))\n",
    "    false_neg = defaultdict(list)\n",
    "    for s, b, e in spans_gold - spans_pred:\n",
    "        false_neg[s].append((b, e))\n",
    "    for row in df.drop_duplicates('sentence_id').itertuples():\n",
    "        if row.sentence_id in false_pos or row.sentence_id in false_neg:\n",
    "            print('Sentence:', row.sentence)\n",
    "            for b, e in false_pos[row.sentence_id]:\n",
    "                print('  FP:', ' '.join(row.sentence.split()[b:e]))\n",
    "            for b, e in false_neg[row.sentence_id]:\n",
    "                print('  FN:', ' '.join(row.sentence.split()[b:e]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1\n",
    "\n",
    "1. Use the `error_report()` function above to inspect and analyse the errors that the automated prediction makes. Base your analysis on the first 500 rows of the training data. Can you see any patterns?\n",
    "2. Summarize your observations in a short text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f6189a9fb722a164a923a3a51bbbd92",
     "grade": true,
     "grade_id": "cell-a17e92a61aedec84",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .\n",
      "  FN: LEICESTERSHIRE\n",
      "Sentence: LONDON 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .\n",
      "  FP: Friday\n",
      "  FP: 38\n",
      "  FP: two days\n",
      "  FP: 39\n",
      "  FP: four\n",
      "  FN: Somerset\n",
      "Sentence: Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .\n",
      "  FP: Essex , Derbyshire and\n",
      "  FN: Essex\n",
      "  FN: Derbyshire\n",
      "Sentence: After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .\n",
      "  FP: 296\n",
      "  FP: three\n",
      "  FP: 83\n",
      "  FP: the opening morning\n",
      "  FP: 83\n",
      "  FP: 94\n",
      "  FP: first\n",
      "  FN: Leicestershire\n",
      "Sentence: Trailing by 213 , Somerset got a solid start to their second innings before Simmons stepped in to bundle them out for 174 .\n",
      "  FP: 213\n",
      "  FP: second\n",
      "  FP: 174\n",
      "Sentence: Essex , however , look certain to regain their top spot after Nasser Hussain and Peter Such gave them a firm grip on their match against Yorkshire at Headingley .\n",
      "  FN: Essex\n",
      "Sentence: Hussain , considered surplus to England 's one-day requirements , struck 158 , his first championship century of the season , as Essex reached 372 and took a first innings lead of 82 .\n",
      "  FP: one-day\n",
      "  FP: 82\n",
      "  FP: first\n",
      "  FP: 372\n",
      "  FP: 158\n",
      "  FP: first\n",
      "  FN: Essex\n",
      "Sentence: By the close Yorkshire had turned that into a 37-run advantage but off-spinner Such had scuttled their hopes , taking four for 24 in 48 balls and leaving them hanging on 119 for five and praying for rain .\n",
      "  FP: four\n",
      "  FP: 24\n",
      "  FP: 119\n",
      "  FP: 48\n",
      "  FP: five\n",
      "  FP: 37-run\n",
      "  FN: Such\n",
      "Sentence: At the Oval , Surrey captain Chris Lewis , another man dumped by England , continued to silence his critics as he followed his four for 45 on Thursday with 80 not out on Friday in the match against Warwickshire .\n",
      "  FP: four\n",
      "  FP: 80\n",
      "  FP: 45\n",
      "  FP: Thursday\n",
      "  FP: Friday\n",
      "  FN: Oval\n",
      "Sentence: He was well backed by England hopeful Mark Butcher who made 70 as Surrey closed on 429 for seven , a lead of 234 .\n",
      "  FP: 429\n",
      "  FP: seven\n",
      "  FP: 234\n",
      "  FP: 70\n",
      "Sentence: Derbyshire kept up the hunt for their first championship title since 1936 by reducing Worcestershire to 133 for five in their second innings , still 100 runs away from avoiding an innings defeat .\n",
      "  FP: first\n",
      "  FP: 1936\n",
      "  FP: second\n",
      "  FP: Worcestershire to 133\n",
      "  FP: five\n",
      "  FP: 100\n",
      "  FN: Worcestershire\n",
      "  FN: Derbyshire\n",
      "Sentence: Australian Tom Moody took six for 82 but Chris Adams , 123 , and Tim O'Gorman , 109 , took Derbyshire to 471 and a first innings lead of 233 .\n",
      "  FP: 123\n",
      "  FP: 471\n",
      "  FP: 233\n",
      "  FP: 109\n",
      "  FP: first\n",
      "  FP: six\n",
      "  FP: 82\n",
      "  FN: Derbyshire\n",
      "Sentence: After the frustration of seeing the opening day of their match badly affected by the weather , Kent stepped up a gear to dismiss Nottinghamshire for 214 .\n",
      "  FP: the opening day\n",
      "  FP: 214\n",
      "Sentence: They were held up by a gritty 84 from Paul Johnson but ex-England fast bowler Martin McCague took four for 55 .\n",
      "  FP: 55\n",
      "  FP: four\n",
      "  FP: 84\n",
      "Sentence: By stumps Kent had reached 108 for three .\n",
      "  FP: 108\n",
      "  FP: three\n",
      "Sentence: CRICKET - ENGLISH COUNTY CHAMPIONSHIP SCORES .\n",
      "  FN: ENGLISH COUNTY CHAMPIONSHIP\n",
      "Sentence: LONDON 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: Result and close of play scores in English county championship matches on Friday :\n",
      "  FP: Friday\n",
      "Sentence: Leicester : Leicestershire beat Somerset by an innings and 39 runs .\n",
      "  FP: 39\n",
      "  FN: Leicester\n",
      "  FN: Leicestershire\n",
      "  FN: Somerset\n",
      "Sentence: Somerset 83 and 174 ( P. Simmons 4-38 ) , Leicestershire 296 .\n",
      "  FP: 174\n",
      "  FP: Leicestershire 296\n",
      "  FP: 83\n",
      "  FP: 4-38\n",
      "  FN: Leicestershire\n",
      "  FN: P. Simmons\n",
      "  FN: Somerset\n",
      "Sentence: Leicestershire 22 points , Somerset 4 .\n",
      "  FP: 22\n",
      "  FP: 4\n",
      "  FN: Leicestershire\n",
      "Sentence: Chester-le-Street : Glamorgan 259 and 207 ( A. Dale 69 , H. Morris 69 ; D. Blenkiron 4-43 ) , Durham 114 ( S. Watkin 4-28 ) and 81-3 .\n",
      "  FP: Durham 114\n",
      "  FP: Glamorgan 259\n",
      "  FP: 81-3\n",
      "  FP: 207\n",
      "  FP: D.\n",
      "  FP: H. Morris 69\n",
      "  FN: S. Watkin\n",
      "  FN: Chester-le-Street\n",
      "  FN: Glamorgan\n",
      "  FN: H. Morris\n",
      "  FN: A. Dale\n",
      "  FN: Durham\n",
      "  FN: D. Blenkiron\n",
      "Sentence: Tunbridge Wells : Nottinghamshire 214 ( P. Johnson 84 ; M. McCague 4-55 ) , Kent 108-3 .\n",
      "  FP: Kent 108-3\n",
      "  FP: 214\n",
      "  FN: M. McCague\n",
      "  FN: Tunbridge Wells\n",
      "  FN: Nottinghamshire\n",
      "  FN: P. Johnson\n",
      "  FN: Kent\n",
      "Sentence: London ( The Oval ) : Warwickshire 195 , Surrey 429-7 ( C. Lewis 80 not out , M. Butcher 70 , G. Kersey 63 , J. Ratcliffe 63 , D. Bicknell 55 ) .\n",
      "  FP: J. Ratcliffe 63\n",
      "  FP: G. Kersey 63\n",
      "  FP: D.\n",
      "  FP: 55\n",
      "  FP: 195\n",
      "  FP: Surrey 429-7\n",
      "  FN: G. Kersey\n",
      "  FN: J. Ratcliffe\n",
      "  FN: The Oval\n",
      "  FN: Surrey\n",
      "  FN: Warwickshire\n",
      "  FN: D. Bicknell\n",
      "  FN: C. Lewis\n",
      "Sentence: Hove : Sussex 363 ( W. Athey 111 , V. Drakes 52 ; I. Austin 4-37 ) , Lancashire 197-8 ( W. Hegg 54 )\n",
      "  FP: W. Hegg 54\n",
      "  FP: W. Athey 111\n",
      "  FP: 363\n",
      "  FP: I. Austin 4-37\n",
      "  FP: 52\n",
      "  FP: Lancashire 197-8\n",
      "  FN: Hove\n",
      "  FN: Sussex\n",
      "  FN: V. Drakes\n",
      "  FN: W. Athey\n",
      "  FN: Lancashire\n",
      "  FN: I. Austin\n",
      "  FN: W. Hegg\n",
      "Sentence: Portsmouth : Middlesex 199 and 426 ( J. Pooley 111 , M. Ramprakash 108 , M. Gatting 83 ) , Hampshire 232 and 109-5 .\n",
      "  FP: Hampshire 232\n",
      "  FP: M. Ramprakash 108\n",
      "  FP: 199\n",
      "  FP: 426\n",
      "  FN: Middlesex\n",
      "  FN: J. Pooley\n",
      "  FN: Portsmouth\n",
      "  FN: M. Ramprakash\n",
      "  FN: Hampshire\n",
      "  FN: M. Gatting\n",
      "Sentence: Chesterfield : Worcestershire 238 and 133-5 , Derbyshire 471 ( J. Adams 123 , T.O'Gorman 109 not out , K. Barnett 87 ; T. Moody 6-82 )\n",
      "  FP: T. Moody 6-82\n",
      "  FP: K. Barnett 87\n",
      "  FP: Worcestershire 238\n",
      "  FP: Derbyshire 471\n",
      "  FP: J. Adams 123\n",
      "  FP: 109\n",
      "  FN: Worcestershire\n",
      "  FN: Derbyshire\n",
      "  FN: T. Moody\n",
      "  FN: T.O'Gorman\n",
      "  FN: J. Adams\n",
      "  FN: K. Barnett\n",
      "Sentence: Bristol : Gloucestershire 183 and 185-6 ( J. Russell 56 not out ) , Northamptonshire 190 ( K. Curran 52 ; A. Smith 5-68 ) .\n",
      "  FP: 183\n",
      "  FP: J. Russell 56\n",
      "  FP: 5-68\n",
      "  FP: 190\n",
      "  FN: J. Russell\n",
      "  FN: K. Curran\n",
      "  FN: Gloucestershire\n",
      "Sentence: CRICKET - 1997 ASHES INTINERARY .\n",
      "  FN: ASHES\n",
      "Sentence: LONDON 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: Australia will defend the Ashes in\n",
      "  FN: Ashes\n",
      "Sentence: starting on May 13 next year , the Test and County Cricket Board\n",
      "  FP: May 13 next year\n",
      "  FN: Test and County Cricket Board\n",
      "Sentence: Australia will also play three one-day internationals and\n",
      "  FP: three\n",
      "Sentence: as well as one-day matches against the Minor Counties and\n",
      "  FP: one-day\n",
      "  FN: Minor Counties\n",
      "Sentence: May 13 Arrive in London\n",
      "  FP: May 13\n",
      "Sentence: May 14 Practice at Lord 's\n",
      "  FP: May 14\n",
      "  FN: Lord 's\n",
      "Sentence: May 15 v Duke of Norfolk 's XI ( at Arundel )\n",
      "  FP: May 15 v Duke of Norfolk 's\n",
      "  FN: Arundel\n",
      "  FN: Duke of Norfolk 's XI\n",
      "Sentence: May 17 v Northampton\n",
      "  FP: May 17\n",
      "Sentence: May 18 v Worcestershire\n",
      "  FP: May 18\n",
      "  FN: Worcestershire\n",
      "Sentence: May 20 v Durham\n",
      "  FP: May 20 v Durham\n",
      "  FN: Durham\n",
      "Sentence: May 22 First one-day international ( at Headingley ,\n",
      "  FP: First\n",
      "  FP: May 22\n",
      "Sentence: May 24 Second one-day international ( at The Oval ,\n",
      "  FP: Second\n",
      "  FP: May 24\n",
      "Sentence: May 25 Third one-day international ( at Lord 's , London )\n",
      "  FP: May 25 Third one-day\n",
      "  FN: Lord 's\n",
      "Sentence: May 27-29 v Gloucestershire or Sussex or Surrey ( three\n",
      "  FP: three\n",
      "  FP: May 27-29\n",
      "  FN: Gloucestershire\n",
      "Sentence: May 31 - June 2 v Derbyshire ( three days )\n",
      "  FP: May 31 - June 2 v Derbyshire\n",
      "  FP: three days\n",
      "  FN: Derbyshire\n",
      "Sentence: June 5-9 First test match ( at Edgbaston , Birmingham )\n",
      "  FP: June 5-9\n",
      "  FP: First\n",
      "Sentence: June 14-16 v Leicestershire ( three days )\n",
      "  FP: June 14-16 v Leicestershire\n",
      "  FP: three days\n",
      "  FN: Leicestershire\n",
      "Sentence: June 19-23 Second test ( at Lord 's )\n",
      "  FP: Second\n",
      "  FP: June 19-23\n",
      "  FN: Lord 's\n",
      "Sentence: June 25-27 v British Universities ( at Oxford , three days )\n",
      "  FP: June 25-27\n",
      "  FP: British\n",
      "  FP: three days\n",
      "  FN: British Universities\n",
      "Sentence: June 28-30 v Hampshire ( three days )\n",
      "  FP: three days\n",
      "  FP: June 28-30 v Hampshire\n",
      "  FN: Hampshire\n",
      "Sentence: July 3-7 Third test ( at Old Trafford , Manchester )\n",
      "  FP: Third\n",
      "  FP: July 3-7\n",
      "Sentence: July 9 v Minor Counties XI\n",
      "  FP: July 9\n",
      "  FN: Minor Counties XI\n",
      "Sentence: July 12 v Scotland\n",
      "  FP: July 12\n",
      "Sentence: July 16-18 v Glamorgan ( three days )\n",
      "  FP: July 16-18\n",
      "  FP: three days\n",
      "Sentence: July 19-21 v Middlesex ( three days )\n",
      "  FP: July 19-21\n",
      "  FP: three days\n",
      "Sentence: July 24-28 Fourth test ( at Headingley )\n",
      "  FP: July 24-28\n",
      "  FP: Fourth\n",
      "Sentence: August 1-4 v Somerset ( four days )\n",
      "  FP: four days\n",
      "  FP: August 1-4\n",
      "Sentence: August 7-11 Fifth test ( at Trent Bridge , Nottingham )\n",
      "  FP: Fifth\n",
      "  FP: August\n",
      "Sentence: August 16-18 v Kent ( three days )\n",
      "  FP: August 16-18\n",
      "  FP: three days\n",
      "Sentence: August 21-25 Sixth test ( at The Oval , London ) .\n",
      "  FP: Sixth\n",
      "  FP: August\n",
      "Sentence: SOCCER - SHEARER NAMED AS ENGLAND CAPTAIN .\n",
      "  FN: ENGLAND\n",
      "  FN: SHEARER\n",
      "Sentence: LONDON 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: The world 's costliest footballer Alan Shearer was named as the new England captain on Friday .\n",
      "  FP: Friday\n",
      "Sentence: The 26-year-old , who joined Newcastle for 15 million pounds sterling ( $ 23.4 million ) , takes over from Tony Adams , who led the side during the European championship in June , and former captain David Platt .\n",
      "  FP: $ 23.4 million\n",
      "  FP: June\n",
      "  FP: 15 million pounds\n",
      "Sentence: Adams and Platt are both injured and will miss England 's opening World Cup qualifier against Moldova on Sunday .\n",
      "  FP: Sunday\n",
      "  FN: Adams\n",
      "  FN: Platt\n",
      "Sentence: Shearer takes the captaincy on a trial basis , but new coach Glenn Hoddle said he saw no reason why the former Blackburn and Southampton skipper should not make the post his own .\n",
      "  FN: Shearer\n",
      "Sentence: \" I 'm sure there wo n't be a problem , I 'm sure Alan is the man for the job , \" Hoddle said .\n",
      "  FN: Hoddle\n",
      "Sentence: \" There were three or four people who could have done it but when I spoke to Alan he was up for it and really wanted it .\n",
      "  FP: three\n",
      "  FP: four\n",
      "Sentence: Shearer 's Euro 96 striking partner Teddy Sheringham withdrew from the squad with an injury on Friday .\n",
      "  FP: Friday\n",
      "  FP: Shearer 's\n",
      "  FN: Shearer\n",
      "Sentence: BELGRADE 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: Red Star ( Yugoslavia ) beat Dinamo ( Russia ) 92-90 ( halftime\n",
      "  FP: 92-90\n",
      "Sentence: SOCCER - ROMANIA BEAT LITHUANIA IN UNDER-21 MATCH .\n",
      "  FN: ROMANIA\n",
      "  FN: LITHUANIA\n",
      "Sentence: BUCHAREST 1996-08-30\n",
      "  FP: BUCHAREST 1996-08-30\n",
      "  FN: BUCHAREST\n",
      "Sentence: Romania beat Lithuania 2-1 ( halftime 1-1 ) in their European under-21 soccer match on Friday .\n",
      "  FP: 2-1\n",
      "  FP: halftime 1-1\n",
      "  FP: Friday\n",
      "Sentence: Romania - Cosmin Contra ( 31st ) , Mihai Tararache ( 75th )\n",
      "  FP: 75th\n",
      "  FP: 31st\n",
      "  FP: Romania - Cosmin Contra\n",
      "  FN: Cosmin Contra\n",
      "  FN: Romania\n",
      "Sentence: Lithuania - Danius Gleveckas ( 13rd )\n",
      "  FP: 13rd\n",
      "  FN: Danius Gleveckas\n",
      "Sentence: SOCCER - ROTOR FANS LOCKED OUT AFTER VOLGOGRAD VIOLENCE .\n",
      "  FN: ROTOR\n",
      "  FN: VOLGOGRAD\n",
      "Sentence: MOSCOW 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: Rotor Volgograd must play their next home game behind closed doors after fans hurled bottles and stones at Dynamo Moscow players during a 1-0 home defeat on Saturday that ended Rotor 's brief spell as league leaders .\n",
      "  FP: Saturday\n",
      "  FP: Volgograd\n",
      "  FN: Rotor\n",
      "  FN: Rotor Volgograd\n",
      "Sentence: The head of the Russian league 's disciplinary committee , Anatoly Gorokhovsky , said on Friday that Rotor would play Lada Togliatti to empty stands on September 3 .\n",
      "  FP: September 3\n",
      "  FP: Friday\n",
      "  FN: Rotor\n",
      "Sentence: The club , who put Manchester United out of last year 's UEFA Cup , were fined $ 1,000 .\n",
      "  FP: 1,000\n",
      "  FP: last year 's\n",
      "Sentence: Despite the defeat , Rotor are well placed with 11 games to play in the championship .\n",
      "  FP: 11\n",
      "  FN: Rotor\n",
      "Sentence: Lying three points behind Alania and two behind Dynamo Moscow , the Volgograd side have a game in hand over the leaders and two over the Moscow club .\n",
      "  FP: two\n",
      "  FP: three\n",
      "  FP: two\n",
      "Sentence: BOXING - PANAMA 'S ROBERTO DURAN FIGHTS THE SANDS OF TIME .\n",
      "  FN: PANAMA\n",
      "  FN: ROBERTO DURAN\n",
      "Sentence: PANAMA CITY 1996-08-30\n",
      "  FP: PANAMA\n",
      "  FN: PANAMA CITY\n",
      "Sentence: Panamanian boxing legend Roberto \" Hands of Stone \" Duran climbs into the ring on Saturday in another age-defying attempt to sustain his long career .\n",
      "  FP: Saturday\n",
      "  FP: Roberto \" Hands of Stone\n",
      "  FP: Duran\n",
      "  FN: Roberto \" Hands of Stone \" Duran\n",
      "Sentence: Duran , 45 , takes on little-known Mexican Ariel Cruz , 30 , in a super middleweight non-title bout in Panama City .\n",
      "  FP: 45\n",
      "  FP: 30\n",
      "Sentence: The fight , Duran 's first on home soil for 10 years , is being billed here as the \" Return of the Legend \" and Duran still talks as if he was in his prime .\n",
      "  FP: first\n",
      "  FP: 10 years\n",
      "Sentence: If he loses Saturday , it could devalue his position as one of the world 's great boxers , \" Panamanian Boxing Association President Ramon Manzanares said .\n",
      "  FP: Saturday\n",
      "  FP: Panamanian Boxing Association\n",
      "  FN: Panamanian\n",
      "  FN: Boxing Association\n",
      "Sentence: Duran , whose 97-12 record spans three decades , hopes a win in the 10-round bout will earn him a rematch against Puerto Rico 's Hector \" Macho \" Camacho .\n",
      "  FP: 97-12\n",
      "  FP: three decades\n",
      "  FP: Puerto Rico 's\n",
      "  FP: 10-round\n",
      "  FN: Hector \" Macho \" Camacho\n",
      "  FN: Puerto Rico\n",
      "Sentence: Camacho took a controversial points decision against the Panamanian in Atlantic City in June in a title fight .\n",
      "  FP: June\n",
      "Sentence: SQUASH - HONG KONG OPEN QUARTER-FINAL RESULTS .\n",
      "  FN: HONG KONG OPEN\n",
      "Sentence: HONG KONG 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: Quarter-final results in the Hong Kong Open on Friday ( prefix number denotes seeding ) : 1 - Jansher Khan ( Pakistan ) beat Mark Cairns ( England ) 15-10 15-6 15-7\n",
      "  FP: 1\n",
      "  FP: Friday\n",
      "  FP: the Hong Kong Open\n",
      "  FP: 15-10 15-6\n",
      "  FN: Hong Kong Open\n",
      "  FN: Jansher Khan\n",
      "Sentence: Anthony Hill ( Australia ) beat Dan Jenson ( Australia ) 15-9 15-8 15-17 17-15\n",
      "  FP: 17-15\n",
      "  FP: 15-9 15-8\n",
      "Sentence: 4 - Peter Nicol ( Scotland ) beat 7 - Chris Walker ( England ) 15-8 15-13 13-15 15-9\n",
      "  FP: 15-9\n",
      "  FP: 4\n",
      "  FP: 7\n",
      "  FN: Peter Nicol\n",
      "  FN: Chris Walker\n",
      "Sentence: 2 - Rodney Eyles ( Australia ) beat Derek Ryan ( Ireland ) 15-6 15-9 11-15 15-10 .\n",
      "  FP: 2 - Rodney Eyles\n",
      "  FP: 11-15 15-10\n",
      "  FN: Rodney Eyles\n",
      "Sentence: SOCCER - RESULTS OF SOUTH KOREAN PRO-SOCCER GAMES .\n",
      "  FN: SOUTH KOREAN\n",
      "Sentence: Pohang 3 Ulsan 2 ( halftime 1-0 )\n",
      "  FP: Pohang 3 Ulsan\n",
      "  FN: Ulsan\n",
      "  FN: Pohang\n",
      "Sentence: Puchon 2 Chonbuk 1 ( halftime 1-1 )\n",
      "  FP: halftime 1-1\n",
      "  FN: Puchon\n",
      "  FN: Chonbuk\n",
      "Sentence: Puchon 3 1 0 6 1 10\n",
      "  FP: 3 1\n",
      "  FN: Puchon\n",
      "Sentence: Chonan 3 0 1 13 10 9\n",
      "  FP: 3\n",
      "  FN: Chonan\n",
      "Sentence: Pohang 2 1 1 11 10 7\n",
      "  FP: 2 1 1\n",
      "  FN: Pohang\n",
      "Sentence: Suwan 1 3 0 7 3 6\n",
      "  FP: 0\n",
      "  FP: Suwan 1 3\n",
      "  FN: Suwan\n",
      "Sentence: Anyang 0 3 1 6 9 3\n",
      "  FP: 0\n",
      "  FN: Anyang\n",
      "Sentence: Chonnam 0 2 1 4 5 2\n",
      "  FN: Chonnam\n",
      "Sentence: Pusan 0 2 1 3 7 2\n",
      "  FN: Pusan\n",
      "Sentence: Chonbuk 0 0 3 3 7 0\n",
      "  FP: 3\n",
      "  FP: 0\n",
      "  FN: Chonbuk\n",
      "Sentence: BASEBALL - RESULTS OF S. KOREAN PROFESSIONAL GAMES .\n",
      "  FN: S. KOREAN\n",
      "Sentence: LG 2 OB 0\n",
      "  FP: 0\n",
      "  FN: LG\n",
      "  FN: OB\n",
      "Sentence: Lotte 6 Hyundai 2\n",
      "  FP: 2\n",
      "  FP: 6\n",
      "  FN: Lotte\n",
      "Sentence: Hyundai 6 Lotte 5\n",
      "  FP: 6\n",
      "  FN: Lotte\n",
      "Sentence: Haitai 2 Samsung 0\n",
      "  FP: Haitai 2 Samsung\n",
      "  FN: Samsung\n",
      "  FN: Haitai\n",
      "Sentence: Samsung 10 Haitai 3\n",
      "  FP: 3\n",
      "  FP: 10\n",
      "  FN: Haitai\n",
      "Sentence: Hanwha 6 Ssangbangwool 5\n",
      "  FN: Ssangbangwool\n",
      "Sentence: Note - Lotte and Hyundai , Haitai and Samsung played two games .\n",
      "  FP: two\n",
      "  FN: Lotte\n",
      "Sentence: Haitai 64 2 43 .596 -\n",
      "  FP: 64\n",
      "Sentence: Ssangbangwool 59 2 49 .545 5 1/2\n",
      "  FP: 59\n",
      "  FP: 5 1/2\n",
      "  FP: 49\n",
      "  FN: Ssangbangwool\n",
      "Sentence: Hanwha 58 1 49 .542 6\n",
      "  FP: 58 1 49\n",
      "Sentence: Hyundai 57 5 49 .536 6 1/2\n",
      "  FP: 5\n",
      "  FP: 57\n",
      "  FP: .536\n",
      "  FP: 6 1/2\n",
      "Sentence: Samsung 49 5 56 .468 14\n",
      "  FP: 49\n",
      "  FP: 5\n",
      "Sentence: Lotte 46 6 54 .462 14 1/2\n",
      "  FP: 14 1/2\n",
      "  FP: 46\n",
      "  FP: 54\n",
      "  FN: Lotte\n",
      "Sentence: LG 46 5 59 .441 17\n",
      "  FP: 17\n",
      "  FP: 46 5\n",
      "  FP: 59\n",
      "Sentence: OB 42 6 62 .409 20 1/2\n",
      "  FP: 62\n",
      "  FP: 42\n",
      "  FP: 1/2\n",
      "  FN: OB\n",
      "Sentence: TENNIS - FRIDAY 'S RESULTS FROM THE U.S. OPEN .\n",
      "  FP: U.S.\n",
      "  FN: U.S. OPEN\n",
      "Sentence: NEW YORK 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: Results from the U.S. Open Tennis Championships at the National Tennis Centre on Friday ( prefix number denotes seeding ) :\n",
      "  FP: the U.S. Open Tennis Championships\n",
      "  FP: Friday\n",
      "  FP: the National Tennis Centre\n",
      "  FN: U.S. Open Tennis Championships\n",
      "  FN: National Tennis Centre\n",
      "Sentence: Sandrine Testud ( France ) beat Ines Gorrochategui ( Argentina ) 4-6 6-2 6-1\n",
      "  FP: 4-6\n",
      "Sentence: 4 - Goran Ivanisevic ( Croatia ) beat Scott Draper ( Australia ) 6-7 ( 1-7 ) 6-3 6-4 6-4\n",
      "  FP: 6-4\n",
      "  FP: 4\n",
      "  FP: 1-7\n",
      "  FN: Goran Ivanisevic\n",
      "Sentence: Tim Henman ( Britain ) beat Doug Flach ( U.S. ) 6-3 6-4 6-2\n",
      "  FP: 6-4\n",
      "Sentence: Mark Philippoussis ( Australia ) beat Andrei Olhovskiy ( Russia ) 6 - 3 6-4 6-2\n",
      "  FP: 6 - 3\n",
      "Sentence: Sjeng Schalken ( Netherlands ) beat David Rikl ( Czech Republic ) 6 - 2 6-4 6-4\n",
      "  FP: 6 - 2\n",
      "  FP: 6-4\n",
      "  FN: Czech Republic\n",
      "Sentence: Guy Forget ( France ) beat 17 - Felix Mantilla ( Spain ) 6-4 7-5 6-3\n",
      "  FP: 17\n",
      "  FP: 6-4\n",
      "  FN: Felix Mantilla\n",
      "Sentence: Alexander Volkov ( Russia ) beat Mikael Tillstrom ( Sweden ) 1-6 6- 4 6-1 4-6 7-6 ( 10-8 )\n",
      "  FP: 1-6\n",
      "  FP: 10-8\n",
      "Sentence: Jonas Bjorkman ( Sweden ) beat David Nainkin ( South Africa ) ) 6-4 6-1 6-1\n",
      "  FP: 6-4 6-1 6-1\n",
      "Sentence: 8 - Lindsay Davenport ( U.S. ) beat Anne-Gaelle Sidot ( France ) 6-0 6-3\n",
      "  FP: 6-3\n",
      "  FP: 8\n",
      "  FN: Lindsay Davenport\n",
      "Sentence: 4 - Conchita Martinez ( Spain ) beat Helena Sukova ( Czech Republic ) 6-4 6-3\n",
      "  FP: 4\n",
      "  FP: 6-4 6-3\n",
      "  FN: Conchita Martinez\n",
      "Sentence: Amanda Coetzer ( South Africa ) beat Irina Spirlea ( Romania ) 7-6 ( 7-5 ) 7-5\n",
      "  FP: 7-5\n",
      "  FP: 7-6\n",
      "Sentence: Add Men 's singles , second round 16 - Cedric Pioline ( France ) beat Roberto Carretero ( Spain ) 4-6 6 - 2 6-2 6-1 Alex Corretja ( Spain ) beat Filippo Veglio ( Switzerland ) 6-7 ( 4- 7 ) 6-4 6-4 6-0\n",
      "  FP: 4-6\n",
      "  FP: 6\n",
      "  FP: 7\n",
      "  FP: Add Men 's\n",
      "  FP: second\n",
      "  FP: 16\n",
      "  FN: Cedric Pioline\n",
      "Sentence: Add Women 's singles , third round Linda Wild ( U.S. ) beat Barbara Rittner ( Germany ) 6-4 4-6 7-5 Asa Carlsson ( Sweden ) beat 15 - Gabriela Sabatini ( Argentina ) 7-5 3-6 6-2\n",
      "  FP: 3-6\n",
      "  FP: third\n",
      "  FP: Carlsson\n",
      "  FP: 6-4\n",
      "  FP: 15\n",
      "  FN: Asa Carlsson\n",
      "  FN: Gabriela Sabatini\n",
      "Sentence: Add Men 's singles , second round 1 - Pete Sampras ( U.S. ) beat Jiri Novak ( Czech Republic ) 6-3 1-6 6-3 4-6 6-4 Paul Haarhuis ( Netherlands ) beat Michael Tebbutt ( Australia ) 1- 6 6-2 6-2 6-3\n",
      "  FP: 6-4\n",
      "  FP: 6-3\n",
      "  FP: 6-3\n",
      "  FP: 1\n",
      "  FP: second\n",
      "  FP: 6\n",
      "  FP: Add Men 's\n",
      "  FP: 6-2 6-3\n",
      "Sentence: Add Women 's singles , third round Lisa Raymond ( U.S. ) beat Kimberly Po ( U.S. ) 6-3 6-2\n",
      "  FP: 6-2\n",
      "  FP: third\n",
      "Sentence: Andrei Medvedev ( Ukraine ) beat Jan Kroslak ( Slovakia ) 6-4 6-3\n",
      "  FP: 6-4\n",
      "Sentence: Petr Korda ( Czech Republic ) bat Bohdan Ulihrach ( Czech Republic ) 6-0 7-6 ( 7-5 ) 6-2\n",
      "  FP: 7-5\n",
      "Sentence: 2 - Monica Seles ( U.S. ) beat Dally Randriantefy ( Madagascar )\n",
      "  FP: 2 - Monica Seles\n",
      "  FP: Dally Randriantefy ( Madagascar\n",
      "  FN: Monica Seles\n",
      "  FN: Dally Randriantefy\n",
      "  FN: Madagascar\n",
      "Sentence: Add men 's singles , second round 12 - Todd Martin ( U.S. ) beat Andrea Gaudenzi ( Italy ) 6-3 6-2 6-2 Stefan Edberg ( Sweden ) beat Bernd Karbacher ( Germany ) 3-6 6-3 6-3 1-0 retired ( leg injury )\n",
      "  FP: 6-2\n",
      "  FP: 12\n",
      "  FP: second\n",
      "  FP: 3-6 6-3\n",
      "Sentence: BASEBALL - MAJOR LEAGUE STANDINGS AFTER THURSDAY 'S GAMES .\n",
      "  FP: THURSDAY\n",
      "  FN: MAJOR LEAGUE\n",
      "Sentence: NEW YORK 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: AMERICAN LEAGUE EASTERN DIVISION\n",
      "  FP: AMERICAN LEAGUE EASTERN\n",
      "  FN: AMERICAN LEAGUE EASTERN DIVISION\n",
      "Sentence: NEW YORK 74 59 .556 -\n",
      "  FP: 74 59\n",
      "  FP: .556 -\n",
      "Sentence: BALTIMORE 70 63 .526 4\n",
      "  FP: 63\n",
      "Sentence: BOSTON 69 65 .515 5 1/2\n",
      "  FP: 69 65\n",
      "  FP: 5 1/2\n",
      "Sentence: TORONTO 63 71 .470 11 1/2\n",
      "  FP: 63\n",
      "  FP: 71\n",
      "  FP: 11 1/2\n",
      "Sentence: DETROIT 48 86 .358 26 1/2\n",
      "  FP: 48\n",
      "  FP: 86\n",
      "  FP: .358\n",
      "Sentence: CENTRAL DIVISION\n",
      "  FP: CENTRAL\n",
      "  FN: CENTRAL DIVISION\n",
      "Sentence: CLEVELAND 80 53 .602 -\n",
      "  FP: 80 53\n",
      "Sentence: CHICAGO 71 64 .526 10\n",
      "  FP: 71 64\n",
      "Sentence: MINNESOTA 67 67 .500 13 1/2\n",
      "  FP: 67 67\n",
      "  FP: .500\n",
      "  FP: 13 1/2\n",
      "  FN: MINNESOTA\n",
      "Sentence: MILWAUKEE 64 71 .474 17\n",
      "  FP: 64 71\n",
      "  FP: 17\n",
      "  FN: MILWAUKEE\n",
      "Sentence: KANSAS CITY 61 74 .452 20\n",
      "  FP: KANSAS\n",
      "  FP: 61 74\n",
      "  FP: 20\n",
      "  FN: KANSAS CITY\n",
      "Sentence: WESTERN DIVISION\n",
      "  FP: WESTERN\n",
      "  FN: WESTERN DIVISION\n",
      "Sentence: TEXAS 75 58 .564 -\n",
      "  FP: 75 58\n",
      "  FP: .564\n",
      "Sentence: SEATTLE 70 63 .526 5\n",
      "  FP: 70 63\n",
      "  FP: 5\n",
      "Sentence: OAKLAND 64 72 .471 12 1/2\n",
      "  FP: 12 1/2\n",
      "  FP: 64 72\n",
      "Sentence: CALIFORNIA 62 72 .463 13 1/2\n",
      "  FP: 62\n",
      "  FP: .463\n",
      "  FP: 72\n",
      "  FP: 13 1/2\n",
      "Sentence: KANSAS CITY AT DETROIT\n",
      "  FP: KANSAS\n",
      "  FN: KANSAS CITY\n",
      "Sentence: CHICAGO AT TORONTO\n",
      "  FN: TORONTO\n",
      "Sentence: MINNESOTA AT MILWAUKEE\n",
      "  FN: MINNESOTA\n",
      "Sentence: NATIONAL LEAGUE EASTERN DIVISION\n",
      "  FP: NATIONAL LEAGUE EASTERN\n",
      "  FN: NATIONAL LEAGUE EASTERN DIVISION\n",
      "Sentence: ATLANTA 83 49 .629 -\n",
      "  FP: 83 49\n",
      "Sentence: MONTREAL 71 61 .538 12\n",
      "  FP: 71 61\n",
      "Sentence: FLORIDA 64 70 .478 20\n",
      "  FP: 20\n",
      "Sentence: NEW YORK 59 75 .440 25\n",
      "  FP: 59 75\n",
      "Sentence: PHILADELPHIA 54 80 .403 30\n",
      "  FP: 54 80\n",
      "Sentence: CENTRAL DIVISION\n",
      "  FP: CENTRAL\n",
      "  FN: CENTRAL DIVISION\n",
      "Sentence: HOUSTON 72 63 .533 -\n",
      "  FP: .533\n",
      "  FP: 72\n",
      "Sentence: ST LOUIS 69 65 .515 2 1/2\n",
      "  FP: 69 65\n",
      "  FP: ST\n",
      "  FP: 2 1/2\n",
      "  FN: ST LOUIS\n",
      "Sentence: CINCINNATI 66 67 .496 5\n",
      "  FP: 67\n",
      "  FP: 66\n",
      "  FP: .496 5\n",
      "Sentence: CHICAGO 65 66 .496 5\n",
      "  FP: 65 66\n",
      "Sentence: PITTSBURGH 56 77 .421 15\n",
      "  FP: 56\n",
      "  FN: PITTSBURGH\n",
      "Sentence: WESTERN DIVISION\n",
      "  FP: WESTERN\n",
      "  FN: WESTERN DIVISION\n",
      "Sentence: SAN DIEGO 75 60 .556 -\n",
      "  FP: 75 60\n",
      "  FN: SAN DIEGO\n",
      "Sentence: LOS ANGELES 72 61 .541 2\n",
      "  FP: 72 61\n",
      "  FP: 2\n",
      "Sentence: COLORADO 70 65 .519 5\n",
      "  FP: 70\n",
      "Sentence: SAN FRANCISCO 57 74 .435 16\n",
      "  FP: 57 74\n",
      "  FN: SAN FRANCISCO\n",
      "Sentence: HOUSTON AT PITTSBURGH\n",
      "  FN: PITTSBURGH\n",
      "Sentence: COLORADO AT ST LOUIS\n",
      "  FN: ST LOUIS\n",
      "Sentence: BASEBALL - MAJOR LEAGUE RESULTS THURSDAY .\n",
      "  FN: MAJOR LEAGUE\n",
      "Sentence: NEW YORK 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: DETROIT 4 Kansas City 1\n",
      "  FP: 4\n",
      "Sentence: Minnesota 6 MILWAUKEE 1\n",
      "  FP: 6\n",
      "  FP: 1\n",
      "  FN: MILWAUKEE\n",
      "Sentence: CALIFORNIA 14 New York 3\n",
      "  FP: 14\n",
      "  FP: 3\n",
      "Sentence: SEATTLE 9 Baltimore 6\n",
      "  FP: 9\n",
      "  FP: 6\n",
      "  FN: Baltimore\n",
      "Sentence: San Diego 3 NEW YORK 2\n",
      "  FP: 2\n",
      "  FP: 3\n",
      "  FN: NEW YORK\n",
      "Sentence: Chicago 4 HOUSTON 3\n",
      "  FP: 3\n",
      "Sentence: Cincinnati 18 COLORADO 7\n",
      "  FP: 18\n",
      "  FN: COLORADO\n",
      "Sentence: Atlanta 5 PITTSBURGH 1\n",
      "  FP: 5\n",
      "  FN: PITTSBURGH\n",
      "Sentence: Los Angeles 2 MONTREAL 1\n",
      "  FP: 2\n",
      "  FP: 1\n",
      "  FN: MONTREAL\n",
      "Sentence: Florida 10 ST LOUIS 9\n",
      "  FP: ST\n",
      "  FP: 10\n",
      "  FN: ST LOUIS\n",
      "Sentence: TENNIS - TARANGO , O'BRIEN SPRING TWIN UPSETS UNDER THE LIGHTS .\n",
      "  FN: TARANGO\n",
      "  FN: O'BRIEN\n",
      "Sentence: NEW YORK 1996-08-30\n",
      "  FP: 1996-08-30\n",
      "Sentence: Andre Agassi escaped disaster on Thursday but Wimbledon finalist MaliVai Washington and Marcelo Rios were not so fortunate on a night of upsets at the U.S. Open .\n",
      "  FP: the U.S. Open\n",
      "  FP: MaliVai\n",
      "  FP: Thursday\n",
      "  FP: Washington\n",
      "  FN: MaliVai Washington\n",
      "  FN: U.S. Open\n",
      "Sentence: The 11th-seeded Washington fell short of reprising his Wimbledon miracle comeback as he lost to red-hot wildcard Alex O'Brien 6-3 6-4 5-7 3-6 6-3 in a two hour 51 minute struggle on the Stadium court .\n",
      "  FP: 6-3\n",
      "  FP: two hour 51 minute\n",
      "  FP: 11th-seeded\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "error_report(df_dev.iloc[:500,:],set(gold_spans(df_dev)),set(pred_spans(df_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "219348824a358db4c99e7388078e3293",
     "grade": true,
     "grade_id": "cell-e4b523a1fe8201ea",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Summary**:\n",
    "\n",
    "Most of errors belong to FP error. Accoding to the analysis result, we can see that most of false positive errors are about \"ORDINAL\",\"DATE\",\"CARDINAL\". These elements are not likely to appear in Wikipedia as entities. From this we can conclude that only entities that are relevant to wikipedia are saved in df_dev dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2\n",
    "\n",
    "Now, use the insights from your error analysis to improve the automated prediction that you implemented in Problem&nbsp;2. While the best way to do this would be to [update spaCy&rsquo;s NER model](https://spacy.io/usage/linguistic-features#updating) using domain-specific training data, for this lab it suffices to write code to post-process the output produced by spaCy. To filter out specific labels it is useful to know the named entity label scheme, which can be found in the [model's documentation](https://spacy.io/models/en#en_core_web_sm). You should be able to improve the F1 score from Problem&nbsp;2 by at least 15 percentage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a20403014f0241dcde59ade54faf2cac",
     "grade": false,
     "grade_id": "cell-0dc8ed778c02fc71",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef pred_spans_improved(df):\\n    \"\"\"Run and evaluate spaCy\\'s NER, with post-processing to improve the results.\\n\\n    Arguments:\\n        df: A data frame.\\n\\n    Yields:\\n        The predicted mention spans in the specified data frame as\\n        triples consisting of the sentence id, start position, and end\\n        position of each span.\\n    \"\"\"\\n    # YOUR CODE HERE\\n    for row in df.itertuples():\\n        doc = nlp(row[2])\\n        for ent in doc.ents:\\n            if ent.label_!= \\'ORDINAL\\'and ent.label_!= \\'DATE\\' and ent.label_ != \\'CARDINAL\\':\\n                yield((row[1],ent.start, ent.end)) \\n\\n                '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def pred_spans_improved(df):\n",
    "    \"\"\"Run and evaluate spaCy's NER, with post-processing to improve the results.\n",
    "\n",
    "    Arguments:\n",
    "        df: A data frame.\n",
    "\n",
    "    Yields:\n",
    "        The predicted mention spans in the specified data frame as\n",
    "        triples consisting of the sentence id, start position, and end\n",
    "        position of each span.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    for row in df.itertuples():\n",
    "        doc = nlp(row[2])\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_!= 'ORDINAL'and ent.label_!= 'DATE' and ent.label_ != 'CARDINAL':\n",
    "                yield((row[1],ent.start, ent.end)) \n",
    "\n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_spans_improved(df):\n",
    "    \"\"\"Run and evaluate spaCy's NER, with post-processing to improve the results.\n",
    "\n",
    "    Arguments:\n",
    "        df: A data frame.\n",
    "\n",
    "    Yields:\n",
    "        The predicted mention spans in the specified data frame as\n",
    "        triples consisting of the sentence id, start position, and end\n",
    "        position of each span.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    for row in df.itertuples():\n",
    "        doc = nlp(row[2])\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_!= 'ORDINAL'and ent.label_!= 'DATE' and ent.label_ != 'CARDINAL' and ent.label_ != 'MONEY' and ent.label_ != 'PERCENT' and ent.label_ != 'QUANTITY' and ent.label_ != 'TIME':\n",
    "                yield((row[1],ent.start, ent.end)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤞 Test your code\n",
    "\n",
    "The following cell reports the evaluation measures from the new function and tests if you achieve the performance goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c399b39793679d0b5f6476fec22442a",
     "grade": true,
     "grade_id": "cell-faf957ffa2d4e385",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.859, Recall: 0.716, F1: 0.781\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\"><strong>Solution appears correct!</strong></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_old = evaluation_scores(spans_dev_gold, spans_dev_pred)\n",
    "scores_new = evaluation_scores(spans_dev_gold, set(pred_spans_improved(df_dev)))\n",
    "print_evaluation_scores(scores_new)\n",
    "assert scores_new[-1] - scores_old[-1] > .15, \"F1-score should improve by at least 15 percentage points.\"\n",
    "success()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3\n",
    "\n",
    "Before moving on, we ask you to store the outputs of the improved named entity recognizer on the development data in a new data frame. This new frame should have the same layout as the original data frame for the development data that you loaded above, but should contain the *predicted* start and end positions for each token span, rather than the gold positions. As the `label` of each span, you can use the special value `--NME--`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fccd0d3a35857ebcbfd0f94adc9b34a",
     "grade": false,
     "grade_id": "cell-ef6dfcb92ae20097",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def df_with_pred_spans(df):\n",
    "    \"\"\"Make a new DataFrame with *predicted* NER spans.\n",
    "\n",
    "    Arguments:\n",
    "        df: A data frame.\n",
    "\n",
    "    Returns:\n",
    "        A *new* data frame with the same layout as `df`t contain, buing\n",
    "        the predicted start and end positions for each token span.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    new_dataframe = pd.DataFrame(columns=[\"sentence_id\",\"sentence\",\"beg\",\"end\",\"label\"])\n",
    "    #list =[]\n",
    "    for row in df.itertuples():\n",
    "        doc = nlp(row[2])\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_!= 'ORDINAL'and ent.label_!= 'DATE' and ent.label_ != 'CARDINAL' and ent.label_ != 'MONEY' and ent.label_ != 'PERCENT' and ent.label_ != 'QUANTITY' and ent.label_ != 'TIME':\n",
    "                sentence_id = row[1]\n",
    "                sentence = row[2]\n",
    "                beg= ent.start\n",
    "                end= ent.end\n",
    "                \n",
    "                new_row = pd.DataFrame([[sentence_id,sentence,beg,end,\"--NNE--\"]],\n",
    "                                       columns=[\"sentence_id\",\"sentence\",\"beg\",\"end\",\"label\"])\n",
    "                #list.append([sentence_id,sentence,beg,end,\"--NNE--\"])\n",
    "                new_dataframe=pd.concat([new_dataframe,new_row])\n",
    "    return (new_dataframe)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤞 Test your code\n",
    "\n",
    "Run the following cell to run your function and display the first few lines of the new data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de25f655cb9cf074f79d8c7a4df038d4",
     "grade": true,
     "grade_id": "cell-45725442562de561",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>beg</th>\n",
       "      <th>end</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0946-001</td>\n",
       "      <td>LONDON 1996-08-30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>--NNE--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>--NNE--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>--NNE--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>--NNE--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0946-002</td>\n",
       "      <td>West Indian all-rounder Phil Simmons took four...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>--NNE--</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_id                                           sentence beg end   \n",
       "0    0946-001                                  LONDON 1996-08-30   0   1  \\\n",
       "0    0946-002  West Indian all-rounder Phil Simmons took four...   0   2   \n",
       "0    0946-002  West Indian all-rounder Phil Simmons took four...   3   5   \n",
       "0    0946-002  West Indian all-rounder Phil Simmons took four...  12  13   \n",
       "0    0946-002  West Indian all-rounder Phil Simmons took four...   0   2   \n",
       "\n",
       "     label  \n",
       "0  --NNE--  \n",
       "0  --NNE--  \n",
       "0  --NNE--  \n",
       "0  --NNE--  \n",
       "0  --NNE--  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_dev_pred = df_with_pred_spans(df_dev)\n",
    "display(df_dev_pred.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'West Indian'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = df_dev_pred.iloc[1]\n",
    "' '.join(row.sentence.split()[row.beg:row.end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Entity linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a method for predicting mention spans, we turn to the task of **entity linking**, which amounts to predicting the knowledge base entity that is referenced by a given mention. In our case, for each span we want to predict the Wikipedia page that this mention references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1\n",
    "\n",
    "Start by extending the generator function that you implemented in Task&nbsp;2.1 to labelled spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1d310571a7ea7d1a9cd36bb33a05742",
     "grade": false,
     "grade_id": "cell-501e0ff440c81adf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gold_mentions(df):\n",
    "    \"\"\"Yield the gold-standard mentions in a data frame.\n",
    "\n",
    "    Args:\n",
    "        df: A data frame.\n",
    "\n",
    "    Yields:\n",
    "        The gold-standard mention spans in the specified data frame as\n",
    "        quadruples consisting of the sentence id, start position, end\n",
    "        position and entity label of each span.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    for row in df.itertuples():\n",
    "        yield((row[1],row[3],row[4],row[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤞 Test your code\n",
    "\n",
    "To test your code, you can run the following cell, which counts the spans yielded by your function when called on the development data (there should still be 5,917 unique tuples, just as in Task 2.1), and checks if some expected tuples are included in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be443ddf411eee7a3e5cf3d63c0987f5",
     "grade": true,
     "grade_id": "cell-2cd2cac3e3b80e0b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\"><strong>Solution appears correct!</strong></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mentions_dev_gold = set(gold_mentions(df_dev))\n",
    "assert len(mentions_dev_gold) == 5917, \"The number of unique returned quadruples should be the same as before.\"\n",
    "assert ('0966-159', 1, 3, '--NME--') in mentions_dev_gold, \"An expected tuple is not included in the results.\"\n",
    "assert ('1094-020', 0, 1, 'Seattle_Mariners') in mentions_dev_gold, \"An expected tuple is not included in the results.\"\n",
    "success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ni = 1\\nfor element in mentions_dev_gold:\\n    print(element)\\n    i+=1\\n    if i >10:\\n        break\\n        '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "i = 1\n",
    "for element in mentions_dev_gold:\n",
    "    print(element)\n",
    "    i+=1\n",
    "    if i >10:\n",
    "        break\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2\n",
    "\n",
    "A naive baseline for entity linking on our data set is to link each mention span to the Wikipedia page name that we get when we join the tokens in the span by underscores, as is standard in Wikipedia page names. Suppose, for example, that a span contains the two tokens\n",
    "\n",
    "    Jimi Hendrix\n",
    "\n",
    "The baseline Wikipedia page name for this span would be\n",
    "\n",
    "    Jimi_Hendrix\n",
    "\n",
    "Implement this naive baseline and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here and in the remainder of this lab, you should base your entity predictions on the predicted spans that you computed in Problem&nbsp;3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5cf0b2e67d61e514d4fecba722dee65e",
     "grade": false,
     "grade_id": "cell-f24799696158afee",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport requests\\ndef baseline(df):\\n    \"\"\"A naive baseline for entity linking that \"predicts\" Wikipedia\\n       page names from the tokens in the mention span.\\n\\n    Arguments:\\n        df: A data frame.\\n\\n    Yields:\\n        The predicted mention spans in the specified data frame as\\n        quadruples consisting of the sentence id, start position, end\\n        position and the predicted entity label of each span.\\n    \"\"\"\\n    # YOUR CODE HERE\\n    for row in df.itertuples():\\n        doc = nlp(row[2])\\n        for ent in doc.ents:\\n            if ent.label_!= \\'ORDINAL\\'and ent.label_!= \\'DATE\\' and ent.label_ != \\'CARDINAL\\':\\n                \\n                # the following part is used to get \"name_wiki\".\\n                kb_id = ent.kb_id_\\n                wikidata_api_url = f\\'https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&ids={kb_id}\\'\\n                response = requests.get(wikidata_api_url)\\n                data = response.json()\\n                name_wiki = data[\\'entities\\'][kb_id][\\'labels\\'][\\'en\\'][\\'value\\'].replace(\" \",\"_\")\\n\\n                yield((row[1],ent.start, ent.end, name_wiki))\\n                '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import requests\n",
    "def baseline(df):\n",
    "    \"\"\"A naive baseline for entity linking that \"predicts\" Wikipedia\n",
    "       page names from the tokens in the mention span.\n",
    "\n",
    "    Arguments:\n",
    "        df: A data frame.\n",
    "\n",
    "    Yields:\n",
    "        The predicted mention spans in the specified data frame as\n",
    "        quadruples consisting of the sentence id, start position, end\n",
    "        position and the predicted entity label of each span.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    for row in df.itertuples():\n",
    "        doc = nlp(row[2])\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_!= 'ORDINAL'and ent.label_!= 'DATE' and ent.label_ != 'CARDINAL':\n",
    "                \n",
    "                # the following part is used to get \"name_wiki\".\n",
    "                kb_id = ent.kb_id_\n",
    "                wikidata_api_url = f'https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&ids={kb_id}'\n",
    "                response = requests.get(wikidata_api_url)\n",
    "                data = response.json()\n",
    "                name_wiki = data['entities'][kb_id]['labels']['en']['value'].replace(\" \",\"_\")\n",
    "\n",
    "                yield((row[1],ent.start, ent.end, name_wiki))\n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(df):\n",
    "    \"\"\"A naive baseline for entity linking that \"predicts\" Wikipedia\n",
    "       page names from the tokens in the mention span.\n",
    "\n",
    "    Arguments:\n",
    "        df: A data frame.\n",
    "\n",
    "    Yields:\n",
    "        The predicted mention spans in the specified data frame as\n",
    "        quadruples consisting of the sentence id, start position, end\n",
    "        position and the predicted entity label of each span.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    for row in df.itertuples():\n",
    "        doc = nlp(row[2])\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_!= 'ORDINAL'and ent.label_!= 'DATE' and ent.label_ != 'CARDINAL' and ent.label_ != 'MONEY' and ent.label_ != 'PERCENT' and ent.label_ != 'QUANTITY' and ent.label_ != 'TIME':\n",
    "                yield((row[1],ent.start, ent.end, '_'.join(row[2].split(' ')[ent.start:ent.end]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test=set(baseline(df_dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dev.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ni = 1\\nfor element in test:\\n    print(element)\\n    i+=1\\n    if i >500:\\n        break \\n        '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "i = 1\n",
    "for element in test:\n",
    "    print(element)\n",
    "    i+=1\n",
    "    if i >500:\n",
    "        break \n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤞 Test your code\n",
    "\n",
    "Again, we can turn to the evaluation measures that we implemented in Problem&nbsp;1.  The expected precision should be around 32%, with an F1-score around 29%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef error_report_2(df, spans_gold, spans_pred):\\n    false_pos = defaultdict(list)\\n    for s, b, e, c in spans_pred - spans_gold:\\n        false_pos[s].append((b, e, c))\\n    false_neg = defaultdict(list)\\n    for s, b, e, c in spans_gold - spans_pred:\\n        false_neg[s].append((b, e, c))\\n    for row in df.drop_duplicates('sentence_id').itertuples():\\n        if row.sentence_id in false_pos or row.sentence_id in false_neg:\\n            print('Sentence:', row.sentence)\\n            for b, e, c in false_pos[row.sentence_id]:\\n                print('  FP:', ' '.join(row.sentence.split()[b:e]),c)\\n            for b, e, c in false_neg[row.sentence_id]:\\n                print('  FN:', ' '.join(row.sentence.split()[b:e]),c)\\n                \""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def error_report_2(df, spans_gold, spans_pred):\n",
    "    false_pos = defaultdict(list)\n",
    "    for s, b, e, c in spans_pred - spans_gold:\n",
    "        false_pos[s].append((b, e, c))\n",
    "    false_neg = defaultdict(list)\n",
    "    for s, b, e, c in spans_gold - spans_pred:\n",
    "        false_neg[s].append((b, e, c))\n",
    "    for row in df.drop_duplicates('sentence_id').itertuples():\n",
    "        if row.sentence_id in false_pos or row.sentence_id in false_neg:\n",
    "            print('Sentence:', row.sentence)\n",
    "            for b, e, c in false_pos[row.sentence_id]:\n",
    "                print('  FP:', ' '.join(row.sentence.split()[b:e]),c)\n",
    "            for b, e, c in false_neg[row.sentence_id]:\n",
    "                print('  FN:', ' '.join(row.sentence.split()[b:e]),c)\n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_report_2(df_dev.iloc[:100,:],mentions_dev_gold,set(baseline(df_dev_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad44b65e17247528d718a6ee4cb19ec3",
     "grade": true,
     "grade_id": "cell-e91c4a73987c75ce",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.315, Recall: 0.263, F1: 0.287\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\"><strong>Solution appears correct!</strong></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = evaluation_scores(mentions_dev_gold, set(baseline(df_dev_pred)))\n",
    "print_evaluation_scores(scores)\n",
    "assert .31 < scores[0] < .32, \"Precision should be between 31% and 32%.\"\n",
    "assert .28 < scores[-1] < .29, \"F1-score should be between 28% and 29%.\"\n",
    "success()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Extending the training data using the knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State-of-the-art approaches to entity linking exploit information in knowledge bases. In our case, where Wikipedia is the knowledge base, one particularly useful type of information are links to other Wikipedia pages. In particular, we can interpret the anchor texts (the highlighted texts that you click on) as mentions of the entities (pages) that they link to. This allows us to harvest long lists of mention–entity pairings.\n",
    "\n",
    "The following cell loads a data frame summarizing anchor texts and page references harvested from the first paragraphs of the English Wikipedia. The data frame also contains all entity mentions in the training data (but not the development or the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d94a9d6704a04efd335a64af5eff287f",
     "grade": false,
     "grade_id": "cell-4a475ad2cc36d263",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with bz2.open('kb.tsv.bz2', 'rt') as source:\n",
    "    df_kb = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what information is available in this data, the following cell shows the entry for the anchor text `Sweden`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>entity</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17436</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>0.985768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17437</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden_national_football_team</td>\n",
       "      <td>0.014173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17438</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden_men's_national_ice_hockey_team</td>\n",
       "      <td>0.000059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mention                                 entity      prob\n",
       "17436  Sweden                                 Sweden  0.985768\n",
       "17437  Sweden          Sweden_national_football_team  0.014173\n",
       "17438  Sweden  Sweden_men's_national_ice_hockey_team  0.000059"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kb.loc[df_kb.mention == 'Sweden']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each row of the data frame contains a pair $(m, e)$ of a mention $m$ and an entity $e$, as well as the conditional probability $P(e|m)$ for mention $m$ referring to entity $e$. These probabilities were estimated based on the frequencies of mention–entity pairs in the knowledge base. The example shows that the anchor text &lsquo;Sweden&rsquo; is most often used to refer to the entity [Sweden](http://en.wikipedia.org/wiki/Sweden), but in a few cases also to refer to Sweden&rsquo;s national football and ice hockey teams. Note that references are sorted in decreasing order of probability, so that the most probable pairing come first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement an entity linking method that resolves each mention to the most probable entity in the data frame. If the mention is not included in the data frame, you can predict the generic label `--NME--`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>entity</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1972 Munich Games</td>\n",
       "      <td>1972_Summer_Olympics</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mention                entity  prob\n",
       "47  1972 Munich Games  1972_Summer_Olympics   1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kb.loc[df_kb.mention==\"1972 Munich Games\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_probable_method(df, df_kb):\n",
    "    \"\"\"An entity linker that resolves each mention to the most probably entity in a knowledge base.\n",
    "\n",
    "    Arguments:\n",
    "        df: A data frame containing the mention spans.\n",
    "        df_kb: A data frame containing the knowledge base.\n",
    "\n",
    "    Yields:\n",
    "        The predicted mention spans in the specified data frame as\n",
    "        quadruples consisting of the sentence id, start position, end\n",
    "        position and the predicted entity label of each span.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    for row in df.itertuples():\n",
    "        mention = ' '.join(row.sentence.split()[row.beg:row.end])\n",
    "        if len(df_kb.loc[df_kb.mention == mention]) == 0:\n",
    "            label = '--NME--'\n",
    "        else:\n",
    "           label = df_kb.loc[df_kb.mention == mention].entity.iloc[0]\n",
    "        yield(row.sentence_id,row.beg,row.end,label)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "We run the same evaluation as before. The expected precision should now be above 65%, with an F1-score just around 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e46f9439641cb65e7e66405a9d3dd6e8",
     "grade": true,
     "grade_id": "cell-53f6994f788bb86e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.659, Recall: 0.549, F1: 0.599\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\"><strong>Solution appears correct!</strong></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = evaluation_scores(mentions_dev_gold, set(most_probable_method(df_dev_pred, df_kb)))\n",
    "print_evaluation_scores(scores)\n",
    "assert scores[0] > .65, \"Precision should be above 65%.\"\n",
    "assert .59 < scores[-1] < .61, \"F1-score should be around 60%.\"\n",
    "success()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Context-sensitive disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the entity mention &lsquo;Lincoln&rsquo;. The most probable entity for this mention turns out to be [Lincoln, Nebraska](http://en.wikipedia.org/Lincoln,_Nebraska); but in pages about American history, we would be better off to predict [Abraham Lincoln](http://en.wikipedia.org/Abraham_Lincoln). This suggests that we should try to disambiguate between different entity references based on the textual context on the page from which the mention was taken. Your task in this last problem is to implement this idea.\n",
    "\n",
    "Set up a dictionary that contains, for each mention $m$ that can refer to more than one entity $e$, a separate Naive Bayes classifier that is trained to predict the correct entity $e$, given the textual context of the mention. As the prior probabilities of the classifier, choose the probabilities $P(e|m)$ that you used in Problem&nbsp;5. To let you estimate the context-specific probabilities, we have compiled a data set with mention contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f3ddc8258b269f76f3ce95612ec168f",
     "grade": false,
     "grade_id": "cell-bc440951a642c773",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with bz2.open('contexts.tsv.bz2') as source:\n",
    "    df_contexts = pd.read_csv(source, sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data frame contains, for each ambiguous mention $m$ and each knowledge base entity $e$ to which this mention can refer, up to 100 randomly selected contexts in which $m$ is used to refer to $e$. For this data, a **context** is defined as the 5 tokens to the left and the 5 tokens to the right of the mention. Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>entity</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970</td>\n",
       "      <td>UEFA_Champions_League</td>\n",
       "      <td>Cup twice the first in @ and the second in 1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970</td>\n",
       "      <td>FIFA_World_Cup</td>\n",
       "      <td>America 1975 and during the @ and 1978 World C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990 World Cup</td>\n",
       "      <td>1990_FIFA_World_Cup</td>\n",
       "      <td>Manolo represented Spain at the @</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990 World Cup</td>\n",
       "      <td>1990_FIFA_World_Cup</td>\n",
       "      <td>Hašek represented Czechoslovakia at the @ and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990 World Cup</td>\n",
       "      <td>1990_FIFA_World_Cup</td>\n",
       "      <td>renovations in 1989 for the @ The present capa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mention                 entity   \n",
       "0            1970  UEFA_Champions_League  \\\n",
       "1            1970         FIFA_World_Cup   \n",
       "2  1990 World Cup    1990_FIFA_World_Cup   \n",
       "3  1990 World Cup    1990_FIFA_World_Cup   \n",
       "4  1990 World Cup    1990_FIFA_World_Cup   \n",
       "\n",
       "                                             context  \n",
       "0    Cup twice the first in @ and the second in 1983  \n",
       "1  America 1975 and during the @ and 1978 World C...  \n",
       "2                 Manolo represented Spain at the @   \n",
       "3  Hašek represented Czechoslovakia at the @ and ...  \n",
       "4  renovations in 1989 for the @ The present capa...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_contexts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in each context, the position of the mention is indicated by the `@` symbol.\n",
    "\n",
    "From this data frame, it is easy to select the data that you need to train the classifiers – the contexts and corresponding entities for all mentions. To illustrate this, the following cell shows how to select all contexts that belong to the mention &lsquo;Lincoln&rsquo;:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41465    Nebraska Concealed Handgun Permit In @ municip...\n",
       "41466    Lazlo restaurants are located in @ and Omaha C...\n",
       "41467    California Washington Overland Park Kansas @ N...\n",
       "41468    City Missouri Omaha Nebraska and @ Nebraska It...\n",
       "41469    by Sandhills Publishing Company in @ Nebraska USA\n",
       "                               ...                        \n",
       "41609                                      @ Leyton Orient\n",
       "41610                    English division three Swansea @ \n",
       "41611    league membership narrowly edging out @ on goa...\n",
       "41612                                          @ Cambridge\n",
       "41613                                                   @ \n",
       "Name: context, Length: 149, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_contexts.context[df_contexts.mention == 'Lincoln']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention</th>\n",
       "      <th>entity</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70739</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>stayed with Nash at Oerkelljunga @ with Prem Z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70740</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>Ekerö Municipality Stockholm County southeaste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70741</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>foreign relations between Argentina and @ Both...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70742</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>Pettersson July 1903 in Älvsbyn @ 18 November ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70743</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>structure located in Uddevalla Bohuslän @ It i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70935</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden_national_football_team</td>\n",
       "      <td>appearance on November 13 against @ in home de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70936</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden_national_football_team</td>\n",
       "      <td>struck the other two against @ in the last fif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70937</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden_national_football_team</td>\n",
       "      <td>football midfielder who played for @ in the 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70938</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden_national_football_team</td>\n",
       "      <td>The group consisted of hosts @ fellow Scandina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70939</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden_men's_national_ice_hockey_team</td>\n",
       "      <td>@</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mention                                 entity   \n",
       "70739  Sweden                                 Sweden  \\\n",
       "70740  Sweden                                 Sweden   \n",
       "70741  Sweden                                 Sweden   \n",
       "70742  Sweden                                 Sweden   \n",
       "70743  Sweden                                 Sweden   \n",
       "...       ...                                    ...   \n",
       "70935  Sweden          Sweden_national_football_team   \n",
       "70936  Sweden          Sweden_national_football_team   \n",
       "70937  Sweden          Sweden_national_football_team   \n",
       "70938  Sweden          Sweden_national_football_team   \n",
       "70939  Sweden  Sweden_men's_national_ice_hockey_team   \n",
       "\n",
       "                                                 context  \n",
       "70739  stayed with Nash at Oerkelljunga @ with Prem Z...  \n",
       "70740  Ekerö Municipality Stockholm County southeaste...  \n",
       "70741  foreign relations between Argentina and @ Both...  \n",
       "70742  Pettersson July 1903 in Älvsbyn @ 18 November ...  \n",
       "70743  structure located in Uddevalla Bohuslän @ It i...  \n",
       "...                                                  ...  \n",
       "70935  appearance on November 13 against @ in home de...  \n",
       "70936  struck the other two against @ in the last fif...  \n",
       "70937  football midfielder who played for @ in the 19...  \n",
       "70938  The group consisted of hosts @ fellow Scandina...  \n",
       "70939                                                 @   \n",
       "\n",
       "[201 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_contexts[df_contexts.mention == 'Sweden']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the context-sensitive disambiguation method and evaluate its performance.  Do this in two parts, first implementing a function that builds the classifiers _(refer to the text above for a detailed description)_, then implementing a prediction function that uses these classifiers to perform the entity prediction.\n",
    "\n",
    "Here are some more **hints** that may help you along the way:\n",
    "\n",
    "1. The prior probabilities for a Naive Bayes classifier can be specified using the `class_prior` option. You will have to provide the probabilities in the same order as the alphabetically sorted class (entity) names.\n",
    "\n",
    "2. Not all mentions in the knowledge base are ambiguous, and therefore not all mentions have context data. If a mention has only one possible entity, pick that one. If a mention has no entity at all, predict the `--NME--` label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmy_dict = {'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}\\nadditional_dict = {'key4': 'value4', 'key5': 'value5'}\\n\\nmy_dict.update(additional_dict)\\n\\nprint(my_dict)\\n\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "my_dict = {'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}\n",
    "additional_dict = {'key4': 'value4', 'key5': 'value5'}\n",
    "\n",
    "my_dict.update(additional_dict)\n",
    "\n",
    "print(my_dict)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f1ed748e347ab8c43e2d352ed183728",
     "grade": false,
     "grade_id": "cell-22e330f1b6937c8d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def build_entity_classifiers(df_kb, df_contexts):\n",
    "    \"\"\"Build Naive Bayes classifiers for entity prediction.\n",
    "\n",
    "    Arguments:\n",
    "        df_kb: A data frame with the knowledge base.\n",
    "        df_contexts: A data frame with contexts for each mention.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where the keys are mentions and the values are Naive Bayes\n",
    "        classifiers trained to predict the correct entity, given the textual\n",
    "        context of the mention (as described in detail above).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    classifier = {}\n",
    "    \n",
    "    \n",
    "    mentions_list = df_contexts.mention.unique()\n",
    "    \n",
    "    for mention in mentions_list:\n",
    "        data_train = df_contexts[df_contexts['mention'] == mention][['entity','context']]\n",
    "        pr_prior = df_kb.loc[df_kb.mention == mention][['entity','prob']].sort_values(by='entity').prob\n",
    "\n",
    "        clf = make_pipeline(CountVectorizer(),\n",
    "                            MultinomialNB(class_prior=pr_prior))\n",
    "        clf.fit(data_train.context,data_train.entity)\n",
    "        classifier.update({mention:clf})\n",
    "    \n",
    "    return(classifier) \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70739</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>stayed with Nash at Oerkelljunga @ with Prem Z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70740</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Ekerö Municipality Stockholm County southeaste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70741</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>foreign relations between Argentina and @ Both...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70742</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Pettersson July 1903 in Älvsbyn @ 18 November ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70743</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>structure located in Uddevalla Bohuslän @ It i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70935</th>\n",
       "      <td>Sweden_national_football_team</td>\n",
       "      <td>appearance on November 13 against @ in home de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70936</th>\n",
       "      <td>Sweden_national_football_team</td>\n",
       "      <td>struck the other two against @ in the last fif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70937</th>\n",
       "      <td>Sweden_national_football_team</td>\n",
       "      <td>football midfielder who played for @ in the 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70938</th>\n",
       "      <td>Sweden_national_football_team</td>\n",
       "      <td>The group consisted of hosts @ fellow Scandina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70939</th>\n",
       "      <td>Sweden_men's_national_ice_hockey_team</td>\n",
       "      <td>@</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      entity   \n",
       "70739                                 Sweden  \\\n",
       "70740                                 Sweden   \n",
       "70741                                 Sweden   \n",
       "70742                                 Sweden   \n",
       "70743                                 Sweden   \n",
       "...                                      ...   \n",
       "70935          Sweden_national_football_team   \n",
       "70936          Sweden_national_football_team   \n",
       "70937          Sweden_national_football_team   \n",
       "70938          Sweden_national_football_team   \n",
       "70939  Sweden_men's_national_ice_hockey_team   \n",
       "\n",
       "                                                 context  \n",
       "70739  stayed with Nash at Oerkelljunga @ with Prem Z...  \n",
       "70740  Ekerö Municipality Stockholm County southeaste...  \n",
       "70741  foreign relations between Argentina and @ Both...  \n",
       "70742  Pettersson July 1903 in Älvsbyn @ 18 November ...  \n",
       "70743  structure located in Uddevalla Bohuslän @ It i...  \n",
       "...                                                  ...  \n",
       "70935  appearance on November 13 against @ in home de...  \n",
       "70936  struck the other two against @ in the last fif...  \n",
       "70937  football midfielder who played for @ in the 19...  \n",
       "70938  The group consisted of hosts @ fellow Scandina...  \n",
       "70939                                                 @   \n",
       "\n",
       "[201 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_contexts[df_contexts['mention'] == 'Sweden'][['entity','context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de0ba0c98997839dd51d4905d52531b5",
     "grade": false,
     "grade_id": "cell-045c73fff60f8fbf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def extended_dictionary_method(df, classifiers, df_kb):\n",
    "    \"\"\"An entity linker that resolves each mention to the most probably entity in a knowledge base.\n",
    "\n",
    "    Arguments:\n",
    "        df: A data frame containing the mention spans.\n",
    "        classifiers: A dictionary of classifiers as produced by the\n",
    "            `build_entity_classifiers` function.\n",
    "        df_kb: A data frame with the knowledge base. (Should be used\n",
    "            to look up a mention if it doesn't have a classifier.)\n",
    "\n",
    "    Yields:\n",
    "        The predicted mention spans in the specified data frame as\n",
    "        quadruples consisting of the sentence id, start position, end\n",
    "        position and the predicted entity label of each span.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    set_intersection = set(df_kb.mention).intersection(set(df_contexts.mention))\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        idx, id, sentence, beg, end, label = row\n",
    "        mention = ' '.join(sentence.split()[beg:end])\n",
    "        if mention in set(df_kb.mention):\n",
    "            if mention in set_intersection:\n",
    "                classifier = classifiers[mention]\n",
    "                label = classifier.predict([' '.join(sentence.split()[beg-5:end+5])])[0]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                label = df_kb.loc[df_kb.mention==mention,'entity'].iloc[0]\n",
    "                \n",
    "        else:\n",
    "            label ='--NME--'\n",
    "        \n",
    "        yield(id,beg,end,label)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "The cell below shows how your functions should all come together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_dictionary_method(df, classifiers, df_kb):\n",
    "    \"\"\"An entity linker that resolves each mention to the most probably entity in a knowledge base.\n",
    "\n",
    "    Arguments:\n",
    "        df: A data frame containing the mention spans.\n",
    "        classifiers: A dictionary of classifiers as produced by the\n",
    "            `build_entity_classifiers` function.\n",
    "        df_kb: A data frame with the knowledge base. (Should be used\n",
    "            to look up a mention if it doesn't have a classifier.)\n",
    "\n",
    "    Yields:\n",
    "        The predicted mention spans in the specified data frame as\n",
    "        quadruples consisting of the sentence id, start position, end\n",
    "        position and the predicted entity label of each span.\n",
    "    \"\"\"\n",
    "    for row in df.itertuples():\n",
    "        idx,id,sentence,beg,end,label = row\n",
    "        mention = ' '.join(sentence.split()[beg:end])\n",
    "        try:\n",
    "            classfier = classifiers[mention]\n",
    "            tokens = ' '.join(sentence.split())\n",
    "            label = classfier.predict(tokens[beg-5:beg]+['@']+tokens[end:end+5])[0]\n",
    "        except:\n",
    "            try:\n",
    "                label = df_kb.loc[df_kb.mention == mention].entity.iloc[0]\n",
    "            except:\n",
    "                label = \"--NME--\"\n",
    "\n",
    "\n",
    "        yield (id,beg,end,label)\n",
    "\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f8ef55cdf1d2eb45704c4bb9718378c",
     "grade": true,
     "grade_id": "cell-dda4aa4e70cd83fb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "classifiers = build_entity_classifiers(df_kb, df_contexts)\n",
    "mentions_dev_pred_dictionary = set(extended_dictionary_method(df_dev_pred, classifiers, df_kb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the cell below evaluates the results as before. You should expect to see a small (around 1&nbsp;unit) increase in each of precision, recall, and F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test= df_contexts.context[df_contexts.mention == \"Swedish\"][70940]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers[\"Swedish\"].predict([text_test])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f90d46c48c1c5adc21534b8e7bac0074",
     "grade": true,
     "grade_id": "cell-ddde598c1d1aff97",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.659, Recall: 0.549, F1: 0.599\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Precision should be above 67%.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yj313\\Desktop\\LEARNING_ MATERIAL\\Text Mining\\2. Lab Sessions\\l4\\l4\\TM-Lab4.ipynb Cell 96\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yj313/Desktop/LEARNING_%20MATERIAL/Text%20Mining/2.%20Lab%20Sessions/l4/l4/TM-Lab4.ipynb#Y164sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m scores \u001b[39m=\u001b[39m evaluation_scores(mentions_dev_gold, mentions_dev_pred_dictionary)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yj313/Desktop/LEARNING_%20MATERIAL/Text%20Mining/2.%20Lab%20Sessions/l4/l4/TM-Lab4.ipynb#Y164sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m print_evaluation_scores(scores)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yj313/Desktop/LEARNING_%20MATERIAL/Text%20Mining/2.%20Lab%20Sessions/l4/l4/TM-Lab4.ipynb#Y164sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39massert\u001b[39;00m scores[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m.67\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPrecision should be above 67\u001b[39m\u001b[39m%\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yj313/Desktop/LEARNING_%20MATERIAL/Text%20Mining/2.%20Lab%20Sessions/l4/l4/TM-Lab4.ipynb#Y164sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39massert\u001b[39;00m scores[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m.61\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mF1-score should be above 61\u001b[39m\u001b[39m%\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yj313/Desktop/LEARNING_%20MATERIAL/Text%20Mining/2.%20Lab%20Sessions/l4/l4/TM-Lab4.ipynb#Y164sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m success()\n",
      "\u001b[1;31mAssertionError\u001b[0m: Precision should be above 67%."
     ]
    }
   ],
   "source": [
    "scores = evaluation_scores(mentions_dev_gold, mentions_dev_pred_dictionary)\n",
    "print_evaluation_scores(scores)\n",
    "assert scores[0] > .67, \"Precision should be above 67%.\"\n",
    "assert scores[-1] > .61, \"F1-score should be above 61%.\"\n",
    "success()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ccdbd-4375-4d2f-8b1d-f47097ef2e84",
   "metadata": {},
   "source": [
    "**Congratulations on finishing this lab! 👍**\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "➡️ Don't forget to **test that everything runs as expected** before you submit!\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
