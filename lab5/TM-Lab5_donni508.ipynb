{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "‚û°Ô∏è Make sure that you have read the **[rules for hand-in assignments](https://www.ida.liu.se/~TDDE16/exam.en.shtml#handins)** and the **[policy on cheating and plagiarism](https://www.ida.liu.se/~TDDE16/exam.en.shtml#cheating)** before starting with this lab.\n",
    "\n",
    "‚û°Ô∏è Make sure you fill in any cells (and _only_ those cells) that say **`YOUR CODE HERE`** or **YOUR ANSWER HERE**, and do _not_ modify any of the other cells.\n",
    "\n",
    "‚û°Ô∏è **Before you submit your lab, make sure everything runs as expected.** For this, _restart the kernel_ and _run all cells_ from top to bottom. In Jupyter Notebook version 7 or higher, you can do this via \"Run$\\rightarrow$Restart Kernel and Run All Cells...\" in the menu (or the \"‚è©\" button in the toolbar).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L5: Large language models and text summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large language models (LLMs) are deep neural networks trained on the language modelling task over massive amounts of text data.  Their strength comes from encoding text representations in a much more sophisticated way than the bag-of-words or tf‚Äìidf representations we have seen in previous labs.\n",
    "\n",
    "In this lab, you will analyse token and sentence representations from a language model of the ‚ÄúBERT‚Äù family of models, which are trained on the masked language modelling objective.  The first part is about extracting and analysing representations from such a model, while the second part is about applying them to the problem of extractive text summarization. In the third part, you will use a larger, autoregressive language model to perform abstractive text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7807a0c189a6c7ed59c8e31b90cdded6",
     "grade": false,
     "grade_id": "cell-c4776a1666a48ddd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define some helper functions that are used in this notebook\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def success():\n",
    "    display(HTML('<div class=\"alert alert-success\"><strong>Solution appears correct!</strong></div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set for embedding analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared a data set containing a small sample of sentences from DBpedia, each containing the word _record_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cdbd05d89e466f7a560be0b9c1dd6ea",
     "grade": false,
     "grade_id": "cell-59c1ac1d68e6c524",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "\n",
    "with bz2.open(\"dbpedia_record_sample.json.bz2\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    df = pd.read_json(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two labelled columns in the data set: `sentence` (the sentence from DBpedia), and `label` (the category of the DBpedia entry where the sentence is from).  The `label` column can take three values: Company, Album, or Athlete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Polar Music is a Swedish record company founded in 1963 by Stig Anderson and his friend Bengt Bernhag.</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Promotone BV is the record label of the Rolling Stones.</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Southern Lord Records is an American independent record label founded by Greg Anderson in 1998.</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Glasgow Underground Recordings is a house music record label owned by Kevin McKay.</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oriental Star Agencies is a British based record label based in the Balsall Heath area of Birmingham.</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 sentence  \\\n",
       "0  Polar Music is a Swedish record company founded in 1963 by Stig Anderson and his friend Bengt Bernhag.   \n",
       "1                                                 Promotone BV is the record label of the Rolling Stones.   \n",
       "2         Southern Lord Records is an American independent record label founded by Greg Anderson in 1998.   \n",
       "3                      Glasgow Underground Recordings is a house music record label owned by Kevin McKay.   \n",
       "4   Oriental Star Agencies is a British based record label based in the Balsall Heath area of Birmingham.   \n",
       "\n",
       "     label  \n",
       "0  Company  \n",
       "1  Company  \n",
       "2  Company  \n",
       "3  Company  \n",
       "4  Company  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the following sentence as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mute is a British record label.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = df.loc[8, \"sentence\"] # 'Mute is a British record label.'\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Contextualized word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first problem, you will extract and analyze contextualized word embeddings from a small BERT model.\n",
    "\n",
    "BERT is a transformer model trained on the masked language modelling objective.  There are many pre-trained variants of this model; we will be using `bert-tiny`, which is the smallest version of BERT available on the Huggingface Model Hub.  The following cell instantiates the model and its tokenizer using PyTorch and the Huggingface Transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "672cb7434218380ab967c0583dbf05a7",
     "grade": false,
     "grade_id": "cell-048052c956fd1876",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out how the tokenizer works by running our example sentence through it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 20101, 2003, 1037, 2329, 2501, 3830, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list under `input_ids` contains a numerical ID for each token in the original sentence.  (You can ignore the other entries for the purposes of this lab.)  Which tokens do these numbers correspond to?  One way to see this is to call the `.convert_ids_to_tokens()` method, which converts the IDs back to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'mute', 'is', 'a', 'british', 'record', 'label', '.', '[SEP]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(sentence)[\"input_ids\"]\n",
    "tokenizer.convert_ids_to_tokens(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a few things here:\n",
    "- The model lowercases all tokens, i.e., it does not distinguish between _record_ and _Record_.\n",
    "- The sentence is ‚Äúframed‚Äù by two ‚Äúspecial‚Äù tokens, `[CLS]` and `[SEP]`, which are automatically inserted by the tokenizer.\n",
    "\n",
    "Before you continue, inspect the output above and consider which numerical ID the token _record_ corresponds to.  You can confirm your answer by inspecting the tokenizer's vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2501"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab[\"record\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have inspected the tokenizer, let's run the entire model. The easiest way to do this is via a pipeline function, which will automatically run tokenization, feed the input into the model, and return the model output. (It also takes care of efficiently processing multiple sentences, running them through the model in batches.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7728,  1.2887, -2.7374,  ..., -0.5095, -1.1886, -1.6815],\n",
      "         [-1.6172,  1.0720,  0.3852,  ..., -1.7962, -0.9175,  0.9404],\n",
      "         [-1.1106,  1.2433,  0.8192,  ..., -0.9257, -0.4360, -0.6360],\n",
      "         ...,\n",
      "         [-0.2771,  0.7692,  0.7107,  ..., -1.2743, -1.2031, -1.3715],\n",
      "         [ 0.0546,  0.8247,  0.4458,  ..., -1.2864, -1.0671, -1.4372],\n",
      "         [-1.2002,  1.0453,  0.3700,  ..., -1.6469, -0.9462, -0.3946]]])\n",
      "torch.Size([1, 9, 128])\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "extractor = pipeline(\"feature-extraction\", batch_size=512, model=model, tokenizer=tokenizer)\n",
    "output = extractor(sentence, return_tensors=True)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a PyTorch tensor. The first dimension is `1` since we only ran a single sentence through the model, the second dimension corresponds to the number of tokens in our sentence, and the third dimension represents the model's embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Extracting specific embeddings\n",
    "\n",
    "Your first task is to extract contextualized embeddings for the word _record_ from this model, for each sentence in the data set.\n",
    "\n",
    "You should implement a function `extract_embeddings()` that takes a list of sentences and a token (such as _record_) and, for each input sentence, returns the contextualized embedding belonging to that token within that sentence. Some notes:\n",
    "\n",
    "- You can use the `tokenizer` and `extractor` variables we defined above.\n",
    "- You can assume that the `token` will appear exactly once in every sentence.\n",
    "- While you may use PyTorch functions and tensor operations, you do not need them to solve this task ‚Äî no knowledge of PyTorch is required beyond what is already given above in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ed99998dad7ada9dcda5ee17db25950",
     "grade": false,
     "grade_id": "cell-8f7b1923540327d2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_embeddings(sentences, token):\n",
    "    \"\"\"Encodes sentences and for each sentence, returns the vector corresponding to the given token.\n",
    "\n",
    "    Arguments:\n",
    "        sentences (list[str]): The sentences to feed into the model.\n",
    "        token (str): The token to extract the embedding for.\n",
    "\n",
    "    Returns:\n",
    "        A list of vectors, one vector for each input sentence, corresponding to the contextual embedding\n",
    "        of `token` within `sentence`. (This can be either a PyTorch tensor or a NumPy array.)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    outputs = []\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        input_ids = tokenizer(sentence)[\"input_ids\"]\n",
    "        idx = input_ids.index(tokenizer.vocab[token])\n",
    "        output = extractor(sentence, return_tensors=True)\n",
    "\n",
    "        vec = output[0,idx,:]\n",
    "\n",
    "        outputs.append(vec)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "The following cell will run your function on an example sentence from the data set, and check if the returned vector matches the expected result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ceaf5b4e544be2b6caa1c77b27b89b6",
     "grade": true,
     "grade_id": "cell-7e9487031e6a44ee",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-success\"><strong>Solution appears correct!</strong></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vect = extract_embeddings([\"Mute is a British record label.\"], \"record\")\n",
    "assert isinstance(vect[0], (torch.Tensor, np.ndarray)), \"Returned vector should be either a Tensor or a NumPy array\"\n",
    "assert len(vect[0]) == 128, \"An embedding vector from this model should have dimensionality 128\"\n",
    "assert np.isclose(vect[0][0], -0.8532, atol=1e-04), \"First dimension of the correct vector should be approximately -0.8532\"\n",
    "assert np.isclose(vect[0].mean(), -0.0446, atol=1e-04), \"Average of all values in the correct vector should be approximately -0.0446\"\n",
    "success()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Plotting embeddings with t-SNE\n",
    "\n",
    "Your second task is to run the function from Task&nbsp;1.1 on all sentences in the data set and plot the resulting embeddings using t-SNE.\n",
    "\n",
    "First, use the function you created in Task 1.1 to extract the embeddings from all sentences.\n",
    "\n",
    "After running the following cell:\n",
    "\n",
    "- `vectors` should contain one vector for each sentence in the data set, each corresponding to the vector of _record_ within that sentence\n",
    "- `labels` should contain the category labels, so that `labels[i]` is the category of the sentence that `vectors[i]` was extracted from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff5a60a3a6f385d3e2c48ddc617b0db7",
     "grade": true,
     "grade_id": "cell-8334ed51819bdd22",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 531 ms\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "vect12  = extract_embeddings(df.sentence, \"record\")\n",
    "\n",
    "vectors, labels = [], []\n",
    "vectors = vect12\n",
    "labels = df.label\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c11ee444a4752ed682c2364dcc1a0f9",
     "grade": false,
     "grade_id": "cell-f584255510d58f5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity-check if the length of the lists are correct:\n",
    "assert len(vectors) == len(labels) == len(df), \\\n",
    "    \"Vectors and/or labels do not have the correct length; there should be one vector & label per sentence in the data set\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement a function that runs t-SNE on these vectors and plots the resulting two-dimensional points as a scatterplot, using the category labels for the _color_ of each point.  This gives us a way to visualize the vectors with respect to the category labels.\n",
    "\n",
    "_Note:_ You can use either plain Matplotlib or [Seaborn](https://seaborn.pydata.org/index.html) for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1c46f08be2eb4e08764e20a5ed8814d",
     "grade": true,
     "grade_id": "cell-2647656718b47656",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_tsne(vectors, labels, perplexity=30.0, n_iter=1000):\n",
    "    \"\"\"Compute and plot a t-SNE reduction of the given vectors.\n",
    "    \n",
    "    Arguments:\n",
    "        vectors: A list of embedding vectors.\n",
    "        labels: A list of class labels; must have the same length as `vectors`.\n",
    "        perplexity (float): A hyperparameter of the t-SNE algorithm; recommended values\n",
    "            are between 5 and 50, and can result in significantly different results.\n",
    "        n_iter (int): A hyperparameter of the t-SNE algorithm, controlling the maximum\n",
    "            number of iterations of the optimization algorithm.\n",
    "\n",
    "    Returns:\n",
    "        Nothing, but shows the plot.\n",
    "    \"\"\"\n",
    "    tsne = TSNE(verbose=True, perplexity=perplexity, n_iter=n_iter)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "If your implementation is correct, running the following cell should produce a scatterplot with points in three different colors, corresponding to the three different categories in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(vectors, labels, perplexity=30.0, n_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to consider how you would interpret the results; you will need this for the reflection part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Sentence embeddings\n",
    "\n",
    "In Problem 1, we extracted embeddings for one specific _word_; how do we obtain embeddings that represent an entire _sentence_?\n",
    "\n",
    "There are multiple ways to go about this. A technique that can be applied to any model is to simply take the _average of all word vectors_ in a sentence. For BERT-style models, we can do something simpler: we can use the representation from the special `[CLS]` token that is placed at the beginning of each sentence. The model is trained in such a way that the representation for this token should encapsulate the overall content of the input sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your implementation from Task&nbsp;1.1 to extract _sentence embeddings_ for each sentence in the data set, so that they can be plotted with your t-SNE implementation from Task&nbsp;1.2.  After running the following cell, `sentence_vectors` should contain one embedding for each sentence in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb3cc56168e8779f90e2c820a8de5dc7",
     "grade": true,
     "grade_id": "cell-4458b1414ed831b7",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sentence_vectors = []\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§û Test your code\n",
    "\n",
    "If your implementation is correct, running the following cell should produce a scatterplot with the sentence vectors, which should look different from the one you obtained in Task&nbsp;1.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(sentence_vectors, labels, perplexity=30.0, n_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set for summarization\n",
    "\n",
    "In the remainder of this lab, we will look specifically at the task of text summarization. For this, we are using a subset of the CNN/DailyMail&nbsp;3.0.0 data set, a popular data set for summarization composed of news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49bdc6b118662b49349ec70e8cfcaba6",
     "grade": false,
     "grade_id": "cell-1a94e1bd051732a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with bz2.open(\"cnn_dailymail_3.0.0_shorts.json.bz2\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    news_df = pd.read_json(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 5,000 news articles, but since some of the techniques we will explore here can be quite compute-intensive (depending on the hardware used to run this lab), we limit ourselves to a selection of ten articles for the exercises. You are welcome (and encouraged!) to inspect and run your code with other articles from the dataset too, but you should only use this small subset for the submitted solutions.\n",
    "\n",
    "There are two labelled columns in the data set: `article` (the full news article), and `highlights` (highlights from the article, which we will treat as a ‚Äúreference summary‚Äù)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Former French President Nicolas Sarkozy was called in for questioning Tuesday as part of a corruption investigation in connection with an inquiry into alleged abuse of power. Financial prosecutor Eliane Houlette confirmed that Sarkozy was called in around 8 a.m. local time by police in Nanterre. Investigators have been looking into Sarkozy's campaign financing, CNN's French affiliate BFMTV reported. They are trying to establish whether the former President, who led the country from 2007 to 2012, obtained confidential information on legal cases concerning him from top magistrates in exchange for the offer of a prestigious post, the channel said. Under French law, Sarkozy can be held for questioning for a day, which can be extended once by another 24 hours. The situation is unprecedented for a former president, BFMTV reported. His lawyer, Thierry Herzog, and two high-ranking magistrates were called in for questioning in the Paris area Monday, according to BFMTV. Their police custody was extended by another 24 hours Tuesday. Sarkozy left office in May 2012 after he lost in the presidential election to Socialist rival Francois Hollande. In recent months, speculation has grown that he might be positioning himself for a political comeback at the head of his center-right party, the UMP. CNN's Jim Bittermann reported from Paris and Laura Smith-Spark wrote in London.</td>\n",
       "      <td>Former French President Nicolas Sarkozy is called in for questioning, prosecutor says . Investigators have been looking into Sarkozy's campaign financing . They want to establish if he obtained secret information about cases concerning him .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Thailand's King Bhumibol Adulyadej has been hospitalized with a high fever and irregular blood pressure, the palace said Saturday. The king, who was admitted to a hospital in Bangkok on Friday evening, appeared to be improving. \"On Saturday morning, his medical team announced that after being administered antibiotics intravenously, his blood pressure stabilized and his fever dropped,\" the palace said in a statement. The 86-year-old is a member of the Chakri dynasty, which has occupied the Thai throne since the 18th century. The U.S.-born and Swiss-educated king is a deeply revered figure whose unifying appeal stretches across all elements of Thai society, from the rich urban elite to poor provincial farmers. He ascended to the throne in 1946, and has reigned over more than 20 prime ministers, 17 military coups and 17 constitutions. CNN's Kocha Olarn reported from Bangkok, and Faith Karimi wrote and reported from Atlanta.</td>\n",
       "      <td>The king was admitted to a hospital in Bangkok . He has reigned over more than 20 prime ministers .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>A Disney Cruise Line employee has been reported missing during a seven-day cruise off the Mexico's Pacific coast, the company said Thursday. The unidentified worker on board the Disney Wonder failed to appear for a Tuesday shift, Disney spokeswoman Christi Erwin Donnan said. The vessel, which left Los Angeles on Sunday, docked Wednesday in Puerto Vallarta, Mexico, Donnan said. The U.S. Coast Guard, the Mexican navy and the Bahamas Maritime Authority have participated in the search and investigation, Donnan said. The Disney Wonder is registered in the Bahamas. The Mexican navy has been conducting searches since Tuesday, Disney Cruise Line said in a written statement. \"Given the circumstances, we are very concerned about this situation and are doing everything possible to assist with the search effort and investigation,\" it said. The company said it conducted a thorough inspection of the cruise ship. U.S. Coast Guard Petty Officer Pamela Manns said the Mexican navy is leading the search and asked for Coast Guard help early in the effort. The Coast Guard provided long-range search aircraft but is not now actively involved in the search. The FBI is not involved because it does not have jurisdiction, as the ship was off the coast of Mexico flying under a foreign flag, said spokeswoman Laura Eimiller of the agency's Los Angeles office. But the FBI has heard no reports of criminal activity or foul play, she said. CNN's Dave Alsup and Sara Weisfeldt contributed to this report.</td>\n",
       "      <td>A cruise line employee has not been seen since Tuesday . Disney Wonder left Los Angeles on Sunday . Mexican navy and Bahamas Maritime Authority are involved in search .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  article  \\\n",
       "6                                                                                                                    Former French President Nicolas Sarkozy was called in for questioning Tuesday as part of a corruption investigation in connection with an inquiry into alleged abuse of power. Financial prosecutor Eliane Houlette confirmed that Sarkozy was called in around 8 a.m. local time by police in Nanterre. Investigators have been looking into Sarkozy's campaign financing, CNN's French affiliate BFMTV reported. They are trying to establish whether the former President, who led the country from 2007 to 2012, obtained confidential information on legal cases concerning him from top magistrates in exchange for the offer of a prestigious post, the channel said. Under French law, Sarkozy can be held for questioning for a day, which can be extended once by another 24 hours. The situation is unprecedented for a former president, BFMTV reported. His lawyer, Thierry Herzog, and two high-ranking magistrates were called in for questioning in the Paris area Monday, according to BFMTV. Their police custody was extended by another 24 hours Tuesday. Sarkozy left office in May 2012 after he lost in the presidential election to Socialist rival Francois Hollande. In recent months, speculation has grown that he might be positioning himself for a political comeback at the head of his center-right party, the UMP. CNN's Jim Bittermann reported from Paris and Laura Smith-Spark wrote in London.   \n",
       "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Thailand's King Bhumibol Adulyadej has been hospitalized with a high fever and irregular blood pressure, the palace said Saturday. The king, who was admitted to a hospital in Bangkok on Friday evening, appeared to be improving. \"On Saturday morning, his medical team announced that after being administered antibiotics intravenously, his blood pressure stabilized and his fever dropped,\" the palace said in a statement. The 86-year-old is a member of the Chakri dynasty, which has occupied the Thai throne since the 18th century. The U.S.-born and Swiss-educated king is a deeply revered figure whose unifying appeal stretches across all elements of Thai society, from the rich urban elite to poor provincial farmers. He ascended to the throne in 1946, and has reigned over more than 20 prime ministers, 17 military coups and 17 constitutions. CNN's Kocha Olarn reported from Bangkok, and Faith Karimi wrote and reported from Atlanta.   \n",
       "56  A Disney Cruise Line employee has been reported missing during a seven-day cruise off the Mexico's Pacific coast, the company said Thursday. The unidentified worker on board the Disney Wonder failed to appear for a Tuesday shift, Disney spokeswoman Christi Erwin Donnan said. The vessel, which left Los Angeles on Sunday, docked Wednesday in Puerto Vallarta, Mexico, Donnan said. The U.S. Coast Guard, the Mexican navy and the Bahamas Maritime Authority have participated in the search and investigation, Donnan said. The Disney Wonder is registered in the Bahamas. The Mexican navy has been conducting searches since Tuesday, Disney Cruise Line said in a written statement. \"Given the circumstances, we are very concerned about this situation and are doing everything possible to assist with the search effort and investigation,\" it said. The company said it conducted a thorough inspection of the cruise ship. U.S. Coast Guard Petty Officer Pamela Manns said the Mexican navy is leading the search and asked for Coast Guard help early in the effort. The Coast Guard provided long-range search aircraft but is not now actively involved in the search. The FBI is not involved because it does not have jurisdiction, as the ship was off the coast of Mexico flying under a foreign flag, said spokeswoman Laura Eimiller of the agency's Los Angeles office. But the FBI has heard no reports of criminal activity or foul play, she said. CNN's Dave Alsup and Sara Weisfeldt contributed to this report.   \n",
       "\n",
       "                                                                                                                                                                                                                                           highlights  \n",
       "6   Former French President Nicolas Sarkozy is called in for questioning, prosecutor says . Investigators have been looking into Sarkozy's campaign financing . They want to establish if he obtained secret information about cases concerning him .  \n",
       "53                                                                                                                                                The king was admitted to a hospital in Bangkok . He has reigned over more than 20 prime ministers .  \n",
       "56                                                                           A cruise line employee has not been seen since Tuesday . Disney Wonder left Los Angeles on Sunday . Mexican navy and Bahamas Maritime Authority are involved in search .  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [6, 53, 56, 340, 730, 1940, 1983, 2404, 2826, 4673]\n",
    "short_news_df = news_df.iloc[indices]\n",
    "short_news_df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Extractive summarization\n",
    "\n",
    "In this problem, we will produce a summary of a news article by extracting a small number of sentences from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Extracting sentence embeddings\n",
    "\n",
    "In the data set, each news article is given as a single string, so we first need to split it up into sentences before we can compute the sentence embeddings.  Your first task is to implement this with the help of spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb16b4b04e8867be3d324463f9837b13",
     "grade": false,
     "grade_id": "cell-946f1654677db457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d94e8eb825daaed3f39c002ed4cb35f8",
     "grade": false,
     "grade_id": "cell-ff8bb57e1fe5a1ba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_sentences_and_embeddings(text):\n",
    "    \"\"\"Splits the text into sentences and computes embeddings for each of them.\n",
    "\n",
    "    Arguments:\n",
    "        text: The text to process, e.g. an entire news article.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (sentences, sentence_vectors).  `sentences` should be a list of \n",
    "        sentences from the text, while `sentence_vectors` should be a list of\n",
    "        embedding vectors corresponding to these sentences.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "The following cell shows how your function should be called, and sanity-checks the returned value for the first news article: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c582165852937aed375283e4cfdb6ec6",
     "grade": true,
     "grade_id": "cell-24d4917c62190190",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sents, vecs = get_sentences_and_embeddings(short_news_df.iloc[2][\"article\"])\n",
    "assert len(sents) == len(vecs) == 13, \"The news article should produce 13 sentences and, therefore, 13 vectors\"\n",
    "assert sents[4] == 'The Disney Wonder is registered in the Bahamas.', \"The fifth sentence in the article is 'The Disney Wonder is registered in the Bahamas.'\"\n",
    "assert np.isclose(np.asarray(vecs).mean(), -0.0617, atol=1e-04), \"Average of all vectors should be approximately -0.0617\"\n",
    "success()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Extractive summarization with MMR\n",
    "\n",
    "You now have all the necessary inputs to produce an extractive summary of a news article. Write a function that takes a list of sentences and their corresponding embedding vectors, as well as a number of sentences to extract from it. Your function should then implement the **maximum marginal relevance (MMR) algorithm** as follows:\n",
    "\n",
    "1. Initially, your candidate set $C$ contains all sentences in the news article, and the set of selected sentences $S$ is empty.\n",
    "2. As the ‚Äúprofile vector‚Äù $p$, use the **centroid** of all the sentence vectors from the news article.\n",
    "3. Pick the next sentence to extract from $C\\textbackslash S$ using the marginal relevance formula:\n",
    "\n",
    "$$\n",
    "s_i = \\textrm{arg\\,max}_{s \\in C\\textbackslash S} ~\\left( \\textrm{sim}(s, p) - \\textrm{max}_{s_j \\in S} ~\\textrm{sim}(s, s_j) \\right)\n",
    "$$\n",
    "\n",
    "In this formula, ‚Äúsim‚Äù is the plain **cosine similarity** between the vectors.  We recommend you use the [`cosine_similarity` function from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) for this purpose.\n",
    "\n",
    "4. Repeat step 3 until you have extracted $n$ sentences, where $n$ is given as an argument to your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e5da3503e228b07c29ce1af6aaee1f6",
     "grade": true,
     "grade_id": "cell-c5e7eac6c849b557",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def extractive_summary(sentences, vectors, n=3):\n",
    "    \"\"\"Produce an extractive summary from a list of sentences and their vectors.\n",
    "\n",
    "    Arguments:\n",
    "        sentences (list): A list of sentences.\n",
    "        vectors (list): A list of vectors, one for each sentence.\n",
    "        n (int): The number of sentences to extract for the summary.\n",
    "\n",
    "    Returns:\n",
    "        A summary of `n` extracted sentences, as a single string.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "The following cell puts it all together: it extracts the sentences and embeddings for one news article, generates the extractive summary, and displays it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_sentences_and_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ni\\Desktop\\732A81 Text Mining\\2. Lab Sessions\\l5\\TM-Lab5.ipynb Cell 54\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m news \u001b[39m=\u001b[39m short_news_df\u001b[39m.\u001b[39miloc[\u001b[39m3\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y104sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sents, vecs \u001b[39m=\u001b[39m get_sentences_and_embeddings(news[\u001b[39m\"\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y104sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m summary \u001b[39m=\u001b[39m extractive_summary(sents, vecs, n\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y104sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Show the article, highlights, and extracted summary in a table\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_sentences_and_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "news = short_news_df.iloc[3]\n",
    "sents, vecs = get_sentences_and_embeddings(news[\"article\"])\n",
    "summary = extractive_summary(sents, vecs, n=3)\n",
    "\n",
    "# Show the article, highlights, and extracted summary in a table\n",
    "pd.DataFrame({\n",
    "    \"article\": [news[\"article\"]],\n",
    "    \"highlights\": [news[\"highlights\"]],\n",
    "    \"summary\": [summary]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, your summary should look like this:\n",
    "\n",
    "> Blackpool have announced that their former goalkeeper Iain Hesford has passed away at the age of 54. 'Condolences go out to his family at this sad time,' said a statement on the Sky Bet Championship club's official website. Hesford made his debut for the Seasiders in 1977 before going on to play for Sunderland and Hull in a career which spanned three decades.\n",
    "\n",
    "Try it out with different news articles to see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Evaluation of summarization\n",
    "\n",
    "In this problem, you implement a ROUGE metric to automatically compare the extracted summaries against the ‚Äúhighlights‚Äù column of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Implementing ROUGE-2\n",
    "\n",
    "Concretely, you should implement **ROUGE-2**, which is the version of the ROUGE metric based on bigram overlap between a system output and a reference.  In our case, the ‚Äúsystem output‚Äù is the (concatenated) string of sentences from the generated summary, while the ‚Äúreference‚Äù is the `highlights` column from the news data set.  The ROUGE-2 score is the F1-score computed from the **number of overlapping bigrams** compared against the **total number of bigrams** in the system output and the reference.\n",
    "\n",
    "You will need to tokenize the inputs in order to compute the bigram overlap. Different tokenizers may result in different ROUGE scores. For the purpose of this problem, **you should use spaCy to tokenize your input.**  If in doubt, you can refer to [the spaCy 101 documentation on tokenization](https://spacy.io/usage/spacy-101#annotations-token).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe0f46630814a3524ffc1c2c625980ef",
     "grade": false,
     "grade_id": "cell-63aabda8fa7143e9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def rouge_2(system, reference):\n",
    "    \"\"\"Compute the ROUGE-2 score between a system output and a reference.\n",
    "    \n",
    "    Arguments:\n",
    "        system (str): The system output, as a single string.\n",
    "        reference (str): The reference to compare against, as a single string.\n",
    "\n",
    "    Returns:\n",
    "        The F1-score of the ROUGE-2 metric between system output and reference.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "The following cell tests your implementation of ROUGE-2 with some toy examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86938f93d5490326a82014dfb6907bca",
     "grade": true,
     "grade_id": "cell-daf48c74916dfc78",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert rouge_2(\"System output.\", \"Reference summary.\") == 0.0, \"Two strings without any bigram overlap should return a score of zero\"\n",
    "assert rouge_2(\"Two identical strings.\", \"Two identical strings.\") == 1.0, \"Two identical strings should return a score of one\"\n",
    "assert rouge_2(\"This is my summary.\", \"This is the summary.\") == 0.5, \"In this example, when using the correct tokenization, exactly half of the bigrams overlap, so the ROUGE-2 score should be 0.5\"\n",
    "success()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Evaluating extractive summarization against a baseline\n",
    "\n",
    "Now that you have an implementation of an evaluation metric, you can use it to evaluate the extractive summaries on the small set of news articles in `short_news_df`.\n",
    "\n",
    "After running the following cell:\n",
    "- `extractive` should contain the extractive summaries for all articles in `short_news_df`, one summary per article, with **two sentences per summary**.\n",
    "- `extractive_rouge_2` should contain the average ROUGE-2 score for the extractive summaries, when evaluated against the \"highlights\" column in `short_news_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e139b9bda8f52f780d0cbeaee67d9ad9",
     "grade": true,
     "grade_id": "cell-5a61bde18eb3cd8f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "extractive = []            # Summaries of all articles in `short_news_df`\n",
    "extractive_rouge_2 = ...   # Average ROUGE-2 score\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot say much about the ROUGE-2 score unless we compare it against a **baseline**.  A simple baseline for text summarization is to just take the first $n$&nbsp;sentences from the article.  In the following cell, you should compute the ROUGE-2 score of this baseline, i.e., taking the **first two sentences** of the article as your ‚Äúsummary‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc7d7ccd44592a5381d72bc7ea934a2b",
     "grade": true,
     "grade_id": "cell-5820bf4cd617611b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "baseline_rouge_2 = ...     # Average ROUGE-2 score of baseline\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "The following cell prints both ROUGE-2 scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5b2a5f20dbfd69e4e9e99904d7fa8d9",
     "grade": false,
     "grade_id": "cell-e0ac74d7536d6e70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"ROUGE-2 (baseline)  : {baseline_rouge_2:.4f}\")\n",
    "print(f\"ROUGE-2 (extractive): {extractive_rouge_2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Abstractive summarization\n",
    "\n",
    "Finally, we will turn to _abstractive_ summarization, for which we will use a distilled version of a large language model that can be run on CPU.  Concretely, we will run a quantized version of Orca-Mini-3B, which is loaded in the following cell.\n",
    "\n",
    "Please note:\n",
    "\n",
    "- **If you are running this on the LiU computers,** the following cell will use the model file that we already downloaded to a shared course folder.\n",
    "- **If you are running this on your own computer,** the following cell will automatically download the model from Huggingface the first time you run it, which **requires approx.&nbsp;2.4&nbsp;GB of disk space!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: Aryanne/Orca-Mini-3B-gguf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200c6891eea1490e840b62ff1a053a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dea297264d942ffa7dea1b0653f6e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fab151b595a409fa0e58c41e6a7e581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading q5_0-orca-mini-3b.gguf:   0%|          | 0.00/2.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ni\\Desktop\\732A81 Text Mining\\2. Lab Sessions\\l5\\TM-Lab5.ipynb Cell 68\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y124sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing model: \u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y124sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m orca_model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(model_name, model_file\u001b[39m=\u001b[39mmodel_file, model_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama\u001b[39m\u001b[39m\"\u001b[39m, gpu_layers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, hf\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y124sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m orca_tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(orca_model)\n",
      "File \u001b[1;32md:\\Program\\AnaConda\\AnaConda\\envs\\liu-text-mining\\Lib\\site-packages\\ctransformers\\hub.py:268\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, model)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(model, CTransformersModel):\n\u001b[0;32m    262\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    263\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCurrently `AutoTokenizer.from_pretrained` only accepts a model object. Please use:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m  model = AutoModelForCausalLM.from_pretrained(..., hf=True)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m  tokenizer = AutoTokenizer.from_pretrained(model)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m     )\n\u001b[1;32m--> 268\u001b[0m \u001b[39mreturn\u001b[39;00m CTransformersTokenizer(model\u001b[39m.\u001b[39;49m_llm)\n",
      "File \u001b[1;32md:\\Program\\AnaConda\\AnaConda\\envs\\liu-text-mining\\Lib\\site-packages\\ctransformers\\transformers.py:84\u001b[0m, in \u001b[0;36mCTransformersTokenizer.__init__\u001b[1;34m(self, llm, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, llm: LLM, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 84\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     85\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm \u001b[39m=\u001b[39m llm\n",
      "File \u001b[1;32md:\\Program\\AnaConda\\AnaConda\\envs\\liu-text-mining\\Lib\\site-packages\\transformers\\tokenization_utils.py:367\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    365\u001b[0m \u001b[39m# 4. If some of the special tokens are not part of the vocab, we add them, at the end.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[39m# the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following `tokenizers`\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_tokens(\n\u001b[0;32m    368\u001b[0m     [token \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_special_tokens_extended \u001b[39mif\u001b[39;49;00m token \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_added_tokens_encoder],\n\u001b[0;32m    369\u001b[0m     special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    370\u001b[0m )\n\u001b[0;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode_use_source_tokenizer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\Program\\AnaConda\\AnaConda\\envs\\liu-text-mining\\Lib\\site-packages\\transformers\\tokenization_utils.py:467\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._add_tokens\u001b[1;34m(self, new_tokens, special_tokens)\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[39mreturn\u001b[39;00m added_tokens\n\u001b[0;32m    466\u001b[0m \u001b[39m# TODO this is fairly slow to improve!\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m current_vocab \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_vocab()\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    468\u001b[0m new_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(current_vocab)  \u001b[39m# only call this once, len gives the last index + 1\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m new_tokens:\n",
      "File \u001b[1;32md:\\Program\\AnaConda\\AnaConda\\envs\\liu-text-mining\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1677\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_vocab\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1667\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_vocab\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]:\n\u001b[0;32m   1668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1669\u001b[0m \u001b[39m    Returns the vocabulary as a dictionary of token to index.\u001b[39;00m\n\u001b[0;32m   1670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1675\u001b[0m \u001b[39m        `Dict[str, int]`: The vocabulary.\u001b[39;00m\n\u001b[0;32m   1676\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m()\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ctransformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Path to the model on the LiU computers\n",
    "model_name = \"/courses/TDDE16/models/q5_0-orca-mini-3b.gguf\"\n",
    "\n",
    "if os.path.exists(model_name):\n",
    "    # Running on the LiU computers\n",
    "    model_file = model_name\n",
    "else:\n",
    "    # *NOT* running on the LiU computers\n",
    "    model_name = \"Aryanne/Orca-Mini-3B-gguf\"\n",
    "    model_file = \"q5_0-orca-mini-3b.gguf\"\n",
    "\n",
    "print(f\"Using model: {model_name}\")\n",
    "orca_model = AutoModelForCausalLM.from_pretrained(model_name, model_file=model_file, model_type=\"llama\", gpu_layers=0, hf=True)\n",
    "orca_tokenizer = AutoTokenizer.from_pretrained(orca_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar as before, we will define a pipeline to interact with our model, but instead of a `feature-extraction` pipeline, we use a `text-generation` pipeline. We also add a `TextStreamer` to our pipeline, which prints the generated text already while it is being generated.  Let's run it on a toy example just to see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'orca_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ni\\Desktop\\732A81 Text Mining\\2. Lab Sessions\\l5\\TM-Lab5.ipynb Cell 70\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y126sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline, TextStreamer\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y126sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m streamer \u001b[39m=\u001b[39m TextStreamer(orca_tokenizer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y126sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m generate \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39morca_model, tokenizer\u001b[39m=\u001b[39morca_tokenizer, streamer\u001b[39m=\u001b[39mstreamer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y126sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m output \u001b[39m=\u001b[39m generate(\u001b[39m\"\u001b[39m\u001b[39mLink√∂ping is a city in\u001b[39m\u001b[39m\"\u001b[39m, max_new_tokens\u001b[39m=\u001b[39m\u001b[39m23\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'orca_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, TextStreamer\n",
    "\n",
    "streamer = TextStreamer(orca_tokenizer)\n",
    "generate = pipeline(\"text-generation\", model=orca_model, tokenizer=orca_tokenizer, streamer=streamer)\n",
    "output = generate(\"Link√∂ping is a city in\", max_new_tokens=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Prompt construction\n",
    "\n",
    "The Orca models were fine-tuned on a specific _input prompt format_ that looks like this:\n",
    "\n",
    "```\n",
    "### System: You are an AI assistant that follows instruction extremely well.\n",
    "### Human:\n",
    "Write a poem about text mining.\n",
    "### Assistant:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text that follows `### System:` is a _system instruction_ that tells the model ‚Äúhow to behave.‚Äù  The text following `### Human:` is the _human input_, i.e. the actual question we want the model to answer, or task that we want the model to solve.  The prompt ends with `### Assistant:` as an indication that this is where the model-generated text begins.\n",
    "\n",
    "We can run this exact prompt through the model as follows _(feel free to try it with different prompts)_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ni\\Desktop\\732A81 Text Mining\\2. Lab Sessions\\l5\\TM-Lab5.ipynb Cell 73\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y132sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39m### System: You are an AI assistant that follows instruction extremely well.\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y132sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m### Human:\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y132sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mWrite a poem about text mining.\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y132sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m### Assistant:\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y132sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ni/Desktop/732A81%20Text%20Mining/2.%20Lab%20Sessions/l5/TM-Lab5.ipynb#Y132sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m output \u001b[39m=\u001b[39m generate(prompt, max_new_tokens\u001b[39m=\u001b[39m\u001b[39m150\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"### System: You are an AI assistant that follows instruction extremely well.\n",
    "### Human:\n",
    "Write a poem about text mining.\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "output = generate(prompt, max_new_tokens=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task here is to **construct a prompt for text summarization** and test it by running it on one of the news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7d5bed43d4bc241f2c642a8d4bfcf5c",
     "grade": true,
     "grade_id": "cell-0a2e8d41e4961329",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def make_prompt_for_summarization(text):\n",
    "    \"\"\"Construct a prompt that instructs the model to summarize a text.\n",
    "\n",
    "    Arguments:\n",
    "        text (str): The text to summarize.\n",
    "\n",
    "    Returns:\n",
    "        A prompt to feed into an Orca model.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "Test your prompt by running the following cell. It should generate a summary of the first news article in `short_news_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = make_prompt_for_summarization(short_news_df.iloc[0][\"article\"])\n",
    "output = generate(prompt, max_new_tokens=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2: Generating the abstractive summaries & evaluating\n",
    "\n",
    "We have everything in place that's needed to generate the abstractive summaries, compute their ROUGE-2 scores against the \"highlights\" column, and compare the results against both the baseline and the extractive summaries. Let's implement that in the following cell. \n",
    "\n",
    "Since we will be generating output for several news articles at once, we set `streamer.skip_prompt = True` in order to make it only output the generated text, rather than the entire prompt.\n",
    "\n",
    "After running the following cell:\n",
    "- `abstractive` should contain the abstractive summaries for all articles in `short_news_df`, one summary per article.  Note that you should _only append the generated summaries_, not including the prompt!\n",
    "- `abstractive_rouge_2` should contain the average ROUGE-2 score for the abstractive summaries, when evaluated against the \"highlights\" column.\n",
    "\n",
    "<i>Note:</i> This cell should take the longest time to run out of all exercises in this notebook; in our testing, it took around 3‚Äì5 minutes on the lab computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c628f420c740dcf23c5353bf9da39485",
     "grade": true,
     "grade_id": "cell-9960c7a1f1c1d578",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "streamer.skip_prompt = True  # This prevents TextStreamer from printing the entire prompt each time\n",
    "abstractive = []             # Summaries of all articles in `short_news_df`\n",
    "abstractive_rouge_2 = ...    # Average ROUGE-2 score\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "The following cell prints all ROUGE-2 scores, and the cell after that displays a table with the highlights and generated summaries, side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03d19a87db7a42466c7a2f3d8469db72",
     "grade": false,
     "grade_id": "cell-5913076d05206107",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"ROUGE-2 (baseline)   : {baseline_rouge_2:.4f}\")\n",
    "print(f\"ROUGE-2 (extractive) : {extractive_rouge_2:.4f}\")\n",
    "print(f\"ROUGE-2 (abstractive): {abstractive_rouge_2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame({\n",
    "    \"highlights\": short_news_df.loc[:, \"highlights\"],\n",
    "    \"extractive\": extractive,\n",
    "    \"abstractive\": abstractive\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.3: Sampling from the probability distribution\n",
    "\n",
    "By default, the generated text is produced using _greedy search_, which simply picks the most likely candidate word each time. This means running the generation pipeline multiple times will result in the exact same generated text each time.  We can instead use a search strategy that _samples_ from the probability distribution, by setting `do_sample=True` when calling the pipeline.  Several parameters can be used to influence the sampling process, one of which is `temperature`, which is a float that is set to `0.8` by default.\n",
    "\n",
    "Run the following cell multiple times with different values for the `temperature` parameter, and observe if/how the results change.  We recommend that you try out values for temperature in the range $(0, 3)$ and run the generation multiple times with each temperature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try out different values for the \"temperate\" parameter here\n",
    "parameters = dict(do_sample=True, temperature=0.8)\n",
    "\n",
    "# Running the text generator on an example article\n",
    "prompt = make_prompt_for_summarization(short_news_df.iloc[3][\"article\"])\n",
    "output = generate(prompt, max_new_tokens=150, **parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no answer/solution for you to provide in this task, but the reflection part of the lab will refer to this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ccdbd-4375-4d2f-8b1d-f47097ef2e84",
   "metadata": {},
   "source": [
    "**Congratulations on finishing this lab! üëç**\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "‚û°Ô∏è Don't forget to **test that everything runs as expected** before you submit!\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
